{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_central = central_wavelength()\n",
    "nb_fwhm_Arr = nb_fwhm(range(60))\n",
    "w_lya = 1215.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load my QSO catalog\n",
    "\n",
    "filename = '/home/alberto/almacen/Source_cats/QSO_100000_v7_double/'\n",
    "files = glob.glob(filename +'data*')\n",
    "files.sort()\n",
    "fi = []\n",
    "\n",
    "for name in files:\n",
    "    fi.append(pd.read_csv(name))\n",
    "\n",
    "data_qso = pd.concat(fi, axis=0, ignore_index=True)\n",
    "\n",
    "qso_flx = data_qso.to_numpy()[:, 1 : 60 + 1].T\n",
    "qso_err = data_qso.to_numpy()[:, 60 + 1 : 120 + 1].T\n",
    "\n",
    "EW_qso = data_qso['EW0'].to_numpy()\n",
    "qso_zspec = data_qso['z'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load SF catalog\n",
    "\n",
    "filename = '/home/alberto/almacen/Source_cats/LAE_10deg_z2-4_v7_double/'\n",
    "files = glob.glob(filename +'data*')\n",
    "files.sort()\n",
    "fi = []\n",
    "\n",
    "for name in files:\n",
    "    fi.append(pd.read_csv(name))\n",
    "\n",
    "data = pd.concat(fi, axis=0, ignore_index=True)\n",
    "\n",
    "sf_flx = data.to_numpy()[:, 1 : 60 + 1].T\n",
    "sf_err = data.to_numpy()[:, 60 + 1 : 120 + 1].T\n",
    "\n",
    "EW_sf = data['EW0'].to_numpy()\n",
    "sf_zspec = data['z'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_flx = np.hstack((qso_flx, sf_flx))\n",
    "pm_err = np.hstack((qso_err, sf_err))\n",
    "zspec = np.concatenate((qso_zspec, sf_zspec))\n",
    "EW_lya = np.concatenate((EW_qso, EW_sf))\n",
    "\n",
    "N_sf = sf_flx.shape[1]\n",
    "N_qso = qso_flx.shape[1]\n",
    "\n",
    "qso_dL = cosmo.luminosity_distance(qso_zspec).to(u.cm).value\n",
    "sf_dL = cosmo.luminosity_distance(sf_zspec).to(u.cm).value\n",
    "\n",
    "sf_L = data['L_lya'].to_numpy()\n",
    "qso_L = data_qso['L_lya'].to_numpy()\n",
    "\n",
    "sf_flambda = 10 ** sf_L / (4*np.pi * sf_dL **2)\n",
    "qso_flambda = data_qso['F_line']\n",
    "\n",
    "L_lya = np.concatenate((qso_L, sf_L))\n",
    "fline = np.concatenate((qso_flambda, sf_flambda))\n",
    "\n",
    "is_qso = np.concatenate((np.ones(N_qso), np.zeros(N_sf))).astype(bool)\n",
    "\n",
    "N_sources = pm_flx.shape[1]\n",
    "\n",
    "%xdel sf_flx\n",
    "%xdel sf_err\n",
    "%xdel qso_flx\n",
    "%xdel qso_err\n",
    "%xdel sf_zspec\n",
    "%xdel qso_zspec\n",
    "%xdel EW_sf\n",
    "%xdel EW_qso\n",
    "%xdel qso_dL\n",
    "%xdel sf_L\n",
    "%xdel qso_L\n",
    "%xdel sf_flambda\n",
    "%xdel qso_flambda\n",
    "%xdel mock\n",
    "%xdel data\n",
    "%xdel data_qso\n",
    "\n",
    "N_sources = pm_flx.shape[1]\n",
    "\n",
    "mag = flux_to_mag(pm_flx[-2], w_central[-2])\n",
    "mag[np.isnan(mag)] = 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lya search\n",
    "cont_est_lya, cont_err_lya = estimate_continuum(pm_flx, pm_err, IGM_T_correct=True)\n",
    "line = is_there_line(pm_flx, pm_err, cont_est_lya, cont_err_lya, 20)\n",
    "lya_lines, lya_cont_lines, line_widths = identify_lines(\n",
    "    line, pm_flx, pm_err, first=True, return_line_width=True\n",
    ")\n",
    "lya_lines = np.array(lya_lines)\n",
    "\n",
    "# Other lines\n",
    "cont_est_other, cont_err_other = estimate_continuum(pm_flx, pm_err, IGM_T_correct=False)\n",
    "line_other = is_there_line(pm_flx, pm_err, cont_est_other, cont_err_other,\n",
    "    400, obs=True)\n",
    "other_lines = identify_lines(line_other, pm_flx, pm_err)\n",
    "\n",
    "# Compute z\n",
    "z_Arr = np.zeros(N_sources)\n",
    "z_Arr[np.where(np.array(lya_lines) != -1)] =\\\n",
    "    z_NB(np.array(lya_cont_lines)[np.where(np.array(lya_lines) != -1)])\n",
    "\n",
    "nice_z = np.abs(z_Arr - zspec) < 0.12\n",
    "\n",
    "%xdel cont_est_other\n",
    "%xdel cont_err_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_min = 17\n",
    "mag_max = 25\n",
    "\n",
    "nb_min = 3\n",
    "nb_max = 20\n",
    "\n",
    "# Used later!!\n",
    "L_min = 40\n",
    "L_max = 47\n",
    "\n",
    "nbs_to_consider = np.arange(nb_min, nb_max + 1)\n",
    "\n",
    "nb_cut = (np.array(lya_lines) >= nb_min) & (np.array(lya_lines) <= nb_max)\n",
    "\n",
    "z_min = (w_central[nb_min] - nb_fwhm_Arr[nb_min] * 0.5) / w_lya - 1\n",
    "z_max = (w_central[nb_max] + nb_fwhm_Arr[nb_max] * 0.5) / w_lya - 1\n",
    "\n",
    "z_cut = (z_min < z_Arr) & (z_Arr < z_max)\n",
    "zspec_cut = (z_min < zspec) & (zspec < z_max)\n",
    "ew_cut = EW_lya > 20\n",
    "mag_cut = (mag > mag_min) & (mag < mag_max)\n",
    "\n",
    "nice_lya = nice_lya_select(\n",
    "    lya_lines, other_lines, pm_flx, pm_err, cont_est_lya, z_Arr\n",
    ")\n",
    "nice_lya = (nice_lya & z_cut & mag_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EW_nb_Arr, EW_nb_e, L_Arr, L_e_Arr, flambda, flambda_e = EW_L_NB(\n",
    "    pm_flx, pm_err, cont_est_lya, cont_err_lya, z_Arr, lya_lines, N_nb=0\n",
    ")\n",
    "\n",
    "nice_lya = nice_lya & (L_lya > L_min) & (L_lya < L_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_nice_qso = count_true(nice_lya & is_qso)\n",
    "N_nice_qso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The initial features are:\n",
    "- The fluxes of the first 55 NBs\n",
    "- The errors of the first 55 NBs\n",
    "- 4 BB fluxes\n",
    "- 4 BB errors\n",
    "- The estimated L\n",
    "- The estimated z\n",
    "TOTAL = 120 features\n",
    "(PCA to be applied below)\n",
    "'''\n",
    "\n",
    "# The data set is the nice_lya sample\n",
    "NNdata = np.hstack(\n",
    "    (\n",
    "        pm_flx[:55, nice_lya].T,\n",
    "        pm_flx[-4:, nice_lya].T,\n",
    "        pm_err[:55, nice_lya].T / pm_flx[:55, nice_lya].T,\n",
    "        pm_err[-4:, nice_lya].T / pm_flx[-4:, nice_lya].T,\n",
    "        L_Arr[nice_lya].reshape(-1, 1),\n",
    "        z_Arr[nice_lya].reshape(-1, 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Data augmentation by perturbing the fluxes\n",
    "N_sources_NN = 100_000\n",
    "# Select equal number of QSOs and SF\n",
    "sample_sources = np.concatenate(\n",
    "    (\n",
    "        np.random.randint(0, N_nice_qso, int(N_sources_NN / 2)),\n",
    "        np.random.randint(N_nice_qso, NNdata.shape[0], int(N_sources_NN / 2))\n",
    "    )\n",
    ")\n",
    "randN_NB = np.random.normal(size=(N_sources_NN, 55))\n",
    "randN_BB = np.random.normal(size=(N_sources_NN, 4))\n",
    "\n",
    "# Select N_sources_NN samples\n",
    "NNdata = NNdata[sample_sources]\n",
    "\n",
    "# Shuffle data\n",
    "shuffle_idx = np.random.permutation(np.arange(N_sources_NN))\n",
    "NNdata = NNdata[shuffle_idx]\n",
    "\n",
    "NNdata_mags = mag[nice_lya][sample_sources][shuffle_idx]\n",
    "NNdata_L = L_Arr[nice_lya][sample_sources][shuffle_idx]\n",
    "NNdata_L_input = L_lya[nice_lya][sample_sources][shuffle_idx]\n",
    "NNdata_isqso = is_qso[nice_lya][sample_sources][shuffle_idx]\n",
    "\n",
    "NNdata_L_input[np.isnan(NNdata_L_input)] = 0.\n",
    "\n",
    "# Apply perturbation\n",
    "# NNdata[:, :55] += NNdata[:, 55 + 4 : 59 + 55] * NNdata[:, :55] * randN_NB\n",
    "# NNdata[:, 55 : 55 + 4] += NNdata[:, 59 + 55 : 59 + 55 + 4] * NNdata[:, 55 : 55 + 4] * randN_BB\n",
    "\n",
    "# Take logs\n",
    "NNdata[:, :55 + 4] = np.log10(NNdata[:, :55 + 4])\n",
    "\n",
    "NNdata[np.isnan(NNdata)] = -99.\n",
    "\n",
    "# The labels are is_qso\n",
    "NNlabels = is_qso[nice_lya]\n",
    "# And also augmentate\n",
    "NNlabels = NNlabels[sample_sources]\n",
    "# And shuffle\n",
    "NNlabels = NNlabels[shuffle_idx]\n",
    "\n",
    "# Rescale data\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(NNdata)\n",
    "NNdata = mms.transform(NNdata)\n",
    "with open('MLmodels/GB_QSO-SF_scaler.sav', 'wb') as file:\n",
    "    pickle.dump(mms, file)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95, svd_solver='full')\n",
    "\n",
    "pca.fit(NNdata)\n",
    "with open('MLmodels/GB_QSO-SF_pca.sav', 'wb') as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "NNdata = pca.transform(NNdata)\n",
    "\n",
    "# Split dataset\n",
    "NNdata_train, NNdata_test, NNlabels_train, NNlabels_test =\\\n",
    "    train_test_split(NNdata, NNlabels, test_size=0.2, shuffle=False)\n",
    "\n",
    "N_train = len(NNlabels_train)\n",
    "N_test = len(NNlabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'learning_rate': [1e-2, 1e-3],\n",
    "    'min_samples_split': [4],\n",
    "    'min_samples_leaf': [3],\n",
    "    'n_estimators': [200],\n",
    "    'subsample': [0.7, 0.6, 0.5],\n",
    "    'criterion': ['friedman_mse'],\n",
    "    'max_depth': [10, 20, 50, 70],\n",
    "}\n",
    "# Create a based model\n",
    "gb = GradientBoostingRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gb, param_grid=param_grid, \n",
    "    cv=3, n_jobs=-1, verbose=2, refit=True\n",
    ")\n",
    "\n",
    "reg = grid_search.fit(NNdata_train, NNdata_L_input[:N_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_lya_test = NNdata_L_input[N_train:]\n",
    "\n",
    "# # The regressor\n",
    "# best_params = {\n",
    "#     'bootstrap': False,\n",
    "#     'max_depth': 50,\n",
    "#     'max_features': 'sqrt',\n",
    "#     'min_samples_leaf': 3,\n",
    "#     'min_samples_split': 4,\n",
    "#     'n_estimators': 200,\n",
    "#     'verbose': True,\n",
    "#     'n_jobs': -1\n",
    "# }\n",
    "# reg = GradientBoostingRegressor(**best_params)\n",
    "\n",
    "# # Train it\n",
    "# reg.fit(NNdata_train, NNdata_L_input[:N_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reg.best_params_\n",
    "\n",
    "{'bootstrap': False,\n",
    " 'max_depth': 50,\n",
    " 'max_features': 'sqrt',\n",
    " 'min_samples_leaf': 3,\n",
    " 'min_samples_split': 4,\n",
    " 'n_estimators': 200}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Arr_pred = reg.predict(NNdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MLmodels/GB_QSO-SF_regressor.sav', 'wb') as file:\n",
    "    pickle.dump(reg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(maskkk, is_qso, title='', nb_c=-3):\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "    mask = maskkk[is_qso]\n",
    "    Z, x, y = np.histogram2d(\n",
    "        L_lya_test[is_qso][mask], L_Arr_pred[is_qso][mask],\n",
    "        bins=(np.linspace(41, 47, 30), np.linspace(41, 47, 30))\n",
    "    )\n",
    "\n",
    "    H_min = np.amin(Z)\n",
    "    H_max = np.amax(Z)\n",
    "\n",
    "    y_centers = 0.5 * (y[1:] + y[:-1])\n",
    "    x_centers = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "    N_bins = 10000\n",
    "\n",
    "    H_Arr = np.linspace(H_min, H_max, N_bins)[::-1]\n",
    "\n",
    "    fact_up_Arr = np.zeros(N_bins)\n",
    "\n",
    "    TOTAL_H = np.sum(Z)\n",
    "\n",
    "    for iii in range(0, N_bins):\n",
    "\n",
    "        mask = Z > H_Arr[iii]\n",
    "\n",
    "        fact_up_Arr[iii] = np.sum(Z[mask]) / TOTAL_H\n",
    "\n",
    "    H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1 sigma\n",
    "    H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2 sigma\n",
    "    H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 3 sigma\n",
    "\n",
    "    ax.contour(\n",
    "        x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "        colors='C0'\n",
    "    )\n",
    "\n",
    "    mask = ~is_qso & maskkk\n",
    "    Z, x, y = np.histogram2d(\n",
    "        L_lya_test[mask], L_Arr_pred[mask],\n",
    "        bins=(np.linspace(41, 47, 30), np.linspace(41, 47, 30))\n",
    "    )\n",
    "\n",
    "    H_min = np.amin(Z)\n",
    "    H_max = np.amax(Z)\n",
    "\n",
    "    y_centers = 0.5 * (y[1:] + y[:-1])\n",
    "    x_centers = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "    N_bins = 10000\n",
    "\n",
    "    H_Arr = np.linspace(H_min , H_max , N_bins )[::-1]\n",
    "\n",
    "    fact_up_Arr = np.zeros(N_bins)\n",
    "\n",
    "    TOTAL_H = np.sum(Z)\n",
    "\n",
    "    for iii in range(0, N_bins):\n",
    "\n",
    "        mask = Z > H_Arr[iii]\n",
    "\n",
    "        fact_up_Arr[iii] = np.sum(Z[ mask ]) / TOTAL_H\n",
    "\n",
    "    H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1sigma\n",
    "    H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2sigma\n",
    "    H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 2sigma\n",
    "\n",
    "    ax.contour(\n",
    "        x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "        colors='C1'\n",
    "    )\n",
    "\n",
    "    x = np.linspace(40, 48, 100)\n",
    "    ax.plot(x, x, linestyle='--', color='red', label='1:1')\n",
    "\n",
    "    ax.set_ylabel('Retrieved $\\log L$', fontsize=15)\n",
    "    ax.set_xlabel('Real $\\log L$', fontsize=15)\n",
    "\n",
    "    ax.set_ylim((41, 47))\n",
    "    ax.set_xlim((41, 47))\n",
    "\n",
    "    if len(title) > 0:\n",
    "        ax.set_title(title, fontsize=20)\n",
    "\n",
    "    # Detec lim\n",
    "\n",
    "    detec_lim = np.vstack(\n",
    "        (\n",
    "            pd.read_csv('csv/5sigma_depths_NB.csv', header=None),\n",
    "            pd.read_csv('csv/5sigma_depths_BB.csv', header=None)\n",
    "        )\n",
    "    )[:, 1]\n",
    "\n",
    "    flambda_lim = mag_to_flux(detec_lim[nb_c], w_central[nb_c]) * 3\n",
    "\n",
    "    ew0_lim = 20 # A\n",
    "    z = w_central[nb_c] / 1215.67 - 1\n",
    "    Fline_lim = ew0_lim * flambda_lim * (1 + z)\n",
    "    dL = cosmo.luminosity_distance(z).to(u.cm).value\n",
    "    L_lim = np.log10(Fline_lim * 4*np.pi * dL**2)\n",
    "\n",
    "    ax.axhline(L_lim, ls='--', color='green', label='L limit')\n",
    "\n",
    "    ax.legend(fontsize=15)\n",
    "    # plt.savefig(f'/home/alberto/Desktop/{title}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(L_Arr_pred.astype(bool), NNlabels_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ade4bca3e0042e0da78fecdb82351169c0f2ccedb06a0d7cf7342df8f7e47af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
