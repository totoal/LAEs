{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_central = central_wavelength()\n",
    "nb_fwhm_Arr = nb_fwhm(range(60))\n",
    "w_lya = 1215.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(regname):\n",
    "    '''\n",
    "    The initial features are:\n",
    "    - The fluxes of the first 53 NBs\n",
    "    - The errors of the first 53 NBs\n",
    "    - 4 BB fluxes\n",
    "    - 4 BB errors\n",
    "    - The estimated L\n",
    "    - The estimated z\n",
    "    TOTAL = 120 features\n",
    "    (PCA to be applied below)\n",
    "    '''\n",
    "\n",
    "    # The data set is the nice_lya sample\n",
    "    NNdata = pd.read_csv(f'MLmodels/datasets/dataset_{regname}_train.csv').to_numpy()[:, 1:]\n",
    "    NNdata_L_input = pd.read_csv(f'MLmodels/datasets/tags_{regname}_train.csv').to_numpy()[:, 1:]\n",
    "    NNdata_test = pd.read_csv(f'MLmodels/datasets/dataset_{regname}_test.csv').to_numpy()[:, 1:]\n",
    "    NNlabels_test = pd.read_csv(f'MLmodels/datasets/tags_{regname}_test.csv').to_numpy()[:, 1:]\n",
    "\n",
    "    # Take the relative fluxes to the selected one\n",
    "    NB_lya_position = NB_z(NNdata[:, -1].reshape(-1,))\n",
    "    for i, nb in enumerate(NB_lya_position - 2):\n",
    "        NNdata[i, :53] = (\n",
    "            flux_to_mag(NNdata[i, :53], w_central[2:55])\n",
    "            # - flux_to_mag(NNdata[i, :53][nb], w_central[nb + 2])\n",
    "        )\n",
    "        NNdata[i, 53 : 53 + 3] = flux_to_mag(NNdata[i, 53 : 53 + 3], w_central[-3:])\n",
    "    NB_lya_position = NB_z(NNdata_test[:, -1].reshape(-1,))\n",
    "    for i, nb in enumerate(NB_lya_position - 2):\n",
    "        NNdata_test[i, :53] = (\n",
    "            flux_to_mag(NNdata_test[i, :53], w_central[2:55])\n",
    "            # - flux_to_mag(NNdata_test[i, :53][nb], w_central[nb + 2])\n",
    "        )\n",
    "        NNdata_test[i, 53 : 53 + 3] = flux_to_mag(NNdata_test[i, 53 : 53 + 3], w_central[-3:])\n",
    "    \n",
    "    N_sources_NN = NNdata.shape[0]\n",
    "\n",
    "    # Shuffle data\n",
    "    # shuffle_idx = np.random.permutation(np.arange(N_sources_NN))\n",
    "    shuffle_idx = np.arange(N_sources_NN)\n",
    "    NNdata = NNdata[shuffle_idx]\n",
    "\n",
    "    NNdata_L_input = NNdata_L_input[shuffle_idx].reshape(-1,)\n",
    "    NNdata_L_input[np.isnan(NNdata_L_input)] = 0\n",
    "    NNdata_is_qso = np.ones(N_sources_NN).astype(bool)\n",
    "    NNdata_is_qso[int(N_sources_NN / 2):] = False\n",
    "    NNdata_is_qso = NNdata_is_qso[shuffle_idx]\n",
    "\n",
    "    # Take logs\n",
    "    NNdata[~np.isfinite(NNdata)] = 99.\n",
    "    # NNdata[NNdata > 99.] = 99.\n",
    "\n",
    "    NNdata_test[~np.isfinite(NNdata_test)] = 99.\n",
    "    # NNdata_test[NNdata_test > 99.] = 99.\n",
    "\n",
    "    # Rescale data\n",
    "    mms = MinMaxScaler()\n",
    "    mms.fit(NNdata_test)\n",
    "    NNdata = mms.transform(NNdata)\n",
    "    NNdata_test = mms.transform(NNdata_test)\n",
    "    with open(f'MLmodels/RF{regname}_QSO-SF_scaler.sav', 'wb') as file:\n",
    "        pickle.dump(mms, file)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=0.95, svd_solver='full')\n",
    "\n",
    "    pca.fit(NNdata_test)\n",
    "    with open(f'MLmodels/RF{regname}_QSO-SF_pca.sav', 'wb') as file:\n",
    "        pickle.dump(pca, file)\n",
    "    NNdata = pca.transform(NNdata)\n",
    "    NNdata_test = pca.transform(NNdata_test)\n",
    "\n",
    "    NNdata_train = NNdata\n",
    "    NNlabels_train = NNdata_L_input\n",
    "\n",
    "    NNlabels_train = NNlabels_train.reshape(-1,)\n",
    "    NNlabels_test = NNlabels_test.reshape(-1,)\n",
    "\n",
    "    return NNdata_train, NNdata_test, NNlabels_train, NNlabels_test\n",
    "\n",
    "# regname = 'mag23-24'\n",
    "# NNdata_L_input, NNdata_train, NNdata_test, NNlabels_train, NNlabels_test, NNdata_L_Arr = prepare_dataset(regname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_grid_search(NNdata, NNlabels):\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'bootstrap': [False],\n",
    "        'max_depth': [25, 50, 75, 100, 125],\n",
    "        'max_features': [0.3],\n",
    "        'min_samples_leaf': [2, 4, 6, 8, 10],\n",
    "        'min_samples_split': [4, 6, 8, 15, 25, 50, 75],\n",
    "        'n_estimators': [100]\n",
    "    }\n",
    "    # Create a based model\n",
    "    rf = RandomForestRegressor()\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf, param_grid=param_grid, \n",
    "        cv=KFold(3), n_jobs=-1, pre_dispatch='2*n_jobs',\n",
    "        verbose=3\n",
    "    )\n",
    "\n",
    "    grid_search.fit(NNdata, NNlabels)\n",
    "\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_and_train(mag_min, mag_max):\n",
    "    regname = f'magAll'\n",
    "    regname_simple = f'magAll'\n",
    "    print(f'#### {regname} ####')\n",
    "    NNdata_train, NNdata_test, NNlabels_train, NNlabels_test = prepare_dataset(regname_simple)\n",
    "    \n",
    "    best_params = do_grid_search(NNdata_test, NNlabels_test)\n",
    "\n",
    "    # with open(f'MLmodels/RF{regname_simple}_QSO-SF_regressor.sav', 'rb') as file:\n",
    "    #     reg = pickle.load(file)\n",
    "    #     best_params = reg.get_params()\n",
    "\n",
    "    print('Best params:')\n",
    "    print(best_params)\n",
    "\n",
    "    reg = RandomForestRegressor(**best_params)\n",
    "    reg.set_params(n_estimators=400, n_jobs=-1)\n",
    "    reg.fit(NNdata_test, NNlabels_test)\n",
    "\n",
    "    print(f'Test score = {reg.score(NNdata_test, NNlabels_test)}')\n",
    "    print(f'Train score = {reg.score(NNdata_train, NNlabels_train)}')\n",
    "\n",
    "    with open(f'MLmodels/RF{regname}_QSO-SF_regressor.sav', 'wb') as file:\n",
    "        pickle.dump(reg, file)\n",
    "\n",
    "mag_min_Arr = [15, 23]\n",
    "mag_max_Arr = [23, 23.5]\n",
    "\n",
    "# for mag_min, mag_max in zip(mag_min_Arr, mag_max_Arr):\n",
    "grid_search_and_train(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'MLmodels/RFmag15-23_QSO-SF_regressor.sav', 'rb') as file:\n",
    "    reg = pickle.load(file)\n",
    "    print(reg.get_params())\n",
    "\n",
    "with open(f'MLmodels/RFmag23-23.5_QSO-SF_regressor.sav', 'rb') as file:\n",
    "    reg = pickle.load(file)\n",
    "    print(reg.get_params())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ade4bca3e0042e0da78fecdb82351169c0f2ccedb06a0d7cf7342df8f7e47af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
