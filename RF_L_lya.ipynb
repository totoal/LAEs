{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_central = central_wavelength()\n",
    "nb_fwhm_Arr = nb_fwhm(range(60))\n",
    "w_lya = 1215.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The initial features are:\n",
    "- The fluxes of the first 55 NBs\n",
    "- The errors of the first 55 NBs\n",
    "- 4 BB fluxes\n",
    "- 4 BB errors\n",
    "- The estimated L\n",
    "- The estimated z\n",
    "TOTAL = 120 features\n",
    "(PCA to be applied below)\n",
    "'''\n",
    "\n",
    "# The data set is the nice_lya sample\n",
    "NNdata = pd.read_csv('MLmodels/dataset100_000.csv').to_numpy()[:, 1:]\n",
    "NNdata_L_input = pd.read_csv('MLmodels/tags100_000.csv').to_numpy()[:, 1:]\n",
    "\n",
    "N_sources_NN = NNdata.shape[0]\n",
    "\n",
    "is_qso = np.ones(N_sources_NN).astype(bool)\n",
    "\n",
    "# Shuffle data\n",
    "shuffle_idx = np.random.permutation(np.arange(N_sources_NN))\n",
    "NNdata = NNdata[shuffle_idx]\n",
    "\n",
    "NNdata_L_input = NNdata_L_input[shuffle_idx]\n",
    "NNdata_L_input[np.isnan(NNdata_L_input)] = 0\n",
    "NNdata_is_qso = np.ones(100000).astype(bool)\n",
    "NNdata_is_qso[50000:] = False\n",
    "NNdata_is_qso = NNdata_is_qso[shuffle_idx]\n",
    "\n",
    "# Take logs\n",
    "NNdata[:, :55 + 4] = np.log10(NNdata[:, :55 + 4])\n",
    "\n",
    "NNdata[np.isnan(NNdata)] = -99.\n",
    "NNdata[NNdata > 99.] = 99.\n",
    "\n",
    "# Rescale data\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(NNdata)\n",
    "NNdata = mms.transform(NNdata)\n",
    "with open('MLmodels/RF_QSO-SF_scaler.sav', 'wb') as file:\n",
    "    pickle.dump(mms, file)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95, svd_solver='full')\n",
    "\n",
    "pca.fit(NNdata)\n",
    "with open('MLmodels/RF_QSO-SF_pca.sav', 'wb') as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "NNdata = pca.transform(NNdata)\n",
    "\n",
    "# Split dataset\n",
    "NNdata_train, NNdata_test, NNlabels_train, NNlabels_test =\\\n",
    "    train_test_split(NNdata, NNdata_L_input, test_size=0.2, shuffle=False)\n",
    "\n",
    "NNlabels_train = NNlabels_train.reshape(-1,)\n",
    "NNlabels_test = NNlabels_test.reshape(-1,)\n",
    "\n",
    "N_train = len(NNlabels_train)\n",
    "N_test = len(NNlabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False, True],\n",
    "    'max_depth': [40, 50, 60, 100],\n",
    "    'max_features': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'min_samples_leaf': [2, 3, 4, 5, 10],\n",
    "    'min_samples_split': [3, 4, 5, 10],\n",
    "    'n_estimators': [150]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf, param_grid=param_grid, \n",
    "    cv=5, n_jobs=-1, verbose=3, return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(NNdata_train, NNlabels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_lya_test = NNlabels_test[N_train:]\n",
    "\n",
    "# The regressor\n",
    "# best_params = {\n",
    "#     'bootstrap': False,\n",
    "#     'max_depth': 50,\n",
    "#     'max_features': 'sqrt',\n",
    "#     'min_samples_leaf': 3,\n",
    "#     'min_samples_split': 4,\n",
    "#     'n_estimators': 200,\n",
    "#     'verbose': True,\n",
    "#     'n_jobs': -1\n",
    "# }\n",
    "best_params = grid_search.best_params_\n",
    "reg = RandomForestRegressor(**best_params)\n",
    "reg.set_params(n_estimators=500)\n",
    "\n",
    "# Train it\n",
    "reg.fit(NNdata_train, NNlabels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Arr_pred = reg.predict(NNdata_test)\n",
    "print(f'Score = {reg.score(NNdata_test, NNlabels_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MLmodels/RF_QSO-SF_regressor.sav', 'wb') as file:\n",
    "    pickle.dump(reg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(is_qso, title='', nb_c=-3):\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "    mask = is_qso\n",
    "    Z, x, y = np.histogram2d(\n",
    "        L_lya_test[mask].reshape(-1,), L_Arr_pred[mask],\n",
    "        bins=(np.linspace(41, 47, 30), np.linspace(41, 47, 30))\n",
    "    )\n",
    "\n",
    "    H_min = np.amin(Z)\n",
    "    H_max = np.amax(Z)\n",
    "\n",
    "    y_centers = 0.5 * (y[1:] + y[:-1])\n",
    "    x_centers = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "    N_bins = 10000\n",
    "\n",
    "    H_Arr = np.linspace(H_min, H_max, N_bins)[::-1]\n",
    "\n",
    "    fact_up_Arr = np.zeros(N_bins)\n",
    "\n",
    "    TOTAL_H = np.sum(Z)\n",
    "\n",
    "    for iii in range(0, N_bins):\n",
    "\n",
    "        mask = Z > H_Arr[iii]\n",
    "\n",
    "        fact_up_Arr[iii] = np.sum(Z[mask]) / TOTAL_H\n",
    "\n",
    "    H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1 sigma\n",
    "    H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2 sigma\n",
    "    H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 3 sigma\n",
    "\n",
    "    ax.contour(\n",
    "        x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "        colors='C0'\n",
    "    )\n",
    "\n",
    "    mask = ~is_qso\n",
    "    Z, x, y = np.histogram2d(\n",
    "        L_lya_test[mask].reshape(-1,), L_Arr_pred[mask],\n",
    "        bins=(np.linspace(41, 47, 30), np.linspace(41, 47, 30))\n",
    "    )\n",
    "\n",
    "    H_min = np.amin(Z)\n",
    "    H_max = np.amax(Z)\n",
    "\n",
    "    y_centers = 0.5 * (y[1:] + y[:-1])\n",
    "    x_centers = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "    N_bins = 10000\n",
    "\n",
    "    H_Arr = np.linspace(H_min , H_max , N_bins )[::-1]\n",
    "\n",
    "    fact_up_Arr = np.zeros(N_bins)\n",
    "\n",
    "    TOTAL_H = np.sum(Z)\n",
    "\n",
    "    for iii in range(0, N_bins):\n",
    "\n",
    "        mask = Z > H_Arr[iii]\n",
    "\n",
    "        fact_up_Arr[iii] = np.sum(Z[ mask ]) / TOTAL_H\n",
    "\n",
    "    H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1sigma\n",
    "    H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2sigma\n",
    "    H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 2sigma\n",
    "\n",
    "    ax.contour(\n",
    "        x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "        colors='C1'\n",
    "    )\n",
    "\n",
    "    x = np.linspace(40, 48, 100)\n",
    "    ax.plot(x, x, linestyle='--', color='red', label='1:1')\n",
    "\n",
    "    ax.set_ylabel('Retrieved $\\log L$', fontsize=15)\n",
    "    ax.set_xlabel('Real $\\log L$', fontsize=15)\n",
    "\n",
    "    ax.set_ylim((41, 47))\n",
    "    ax.set_xlim((41, 47))\n",
    "\n",
    "    if len(title) > 0:\n",
    "        ax.set_title(title, fontsize=20)\n",
    "\n",
    "    # Detec lim\n",
    "\n",
    "    detec_lim = np.vstack(\n",
    "        (\n",
    "            pd.read_csv('csv/5sigma_depths_NB.csv', header=None),\n",
    "            pd.read_csv('csv/5sigma_depths_BB.csv', header=None)\n",
    "        )\n",
    "    )[:, 1]\n",
    "\n",
    "    flambda_lim = mag_to_flux(detec_lim[nb_c], w_central[nb_c]) * 3\n",
    "\n",
    "    ew0_lim = 20 # A\n",
    "    z = w_central[nb_c] / 1215.67 - 1\n",
    "    Fline_lim = ew0_lim * flambda_lim * (1 + z)\n",
    "    dL = cosmo.luminosity_distance(z).to(u.cm).value\n",
    "    L_lim = np.log10(Fline_lim * 4*np.pi * dL**2)\n",
    "\n",
    "    ax.axhline(L_lim, ls='--', color='green', label='L limit')\n",
    "\n",
    "    ax.legend(fontsize=15)\n",
    "    # plt.savefig(f'/home/alberto/Desktop/{title}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(NNdata_is_qso[-20000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.set_params(verbose=1)\n",
    "a = learning_curve(reg, NNdata, NNdata_L_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "x_ticks = np.arange(len(a[1]))\n",
    "ax.plot(x_ticks, a[1].sum(axis=1)/len(x_ticks), marker='s', label='train')\n",
    "ax.plot(x_ticks, a[2].sum(axis=1)/len(x_ticks), marker='s', label='test')\n",
    "\n",
    "ax.legend(fontsize=15)\n",
    "\n",
    "ax.set_ylabel('score', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ade4bca3e0042e0da78fecdb82351169c0f2ccedb06a0d7cf7342df8f7e47af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
