{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_central = central_wavelength()\n",
    "nb_fwhm_Arr = nb_fwhm(range(60))\n",
    "w_lya = 1215.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(regname):\n",
    "    '''\n",
    "    The initial features are:\n",
    "    - The fluxes of the first 53 NBs\n",
    "    - The errors of the first 53 NBs\n",
    "    - 4 BB fluxes\n",
    "    - 4 BB errors\n",
    "    - The estimated L\n",
    "    - The estimated z\n",
    "    TOTAL = 120 features\n",
    "    (PCA to be applied below)\n",
    "    '''\n",
    "\n",
    "    # The data set is the nice_lya sample\n",
    "    NNdata = pd.read_csv(f'MLmodels/datasets/dataset_{regname}_5fold_nice_train.csv').to_numpy()[:, 1:]\n",
    "    NNdata_L_input = pd.read_csv(f'MLmodels/datasets/tags_{regname}_5fold_nice_train.csv').to_numpy()[:, 1:]\n",
    "    NNdata_L_Arr = pd.read_csv(f'MLmodels/datasets/dataset_{regname}_test.csv').to_numpy()[:, -2].reshape(-1,)\n",
    "    NNdata_test = pd.read_csv(f'MLmodels/datasets/dataset_{regname}_test.csv').to_numpy()[:, 1:]\n",
    "    NNlabels_test = pd.read_csv(f'MLmodels/datasets/tags_{regname}_test.csv').to_numpy()[:, 1:]\n",
    "    print(NNdata.shape)\n",
    "\n",
    "    # Take the relative fluxes to the selected one\n",
    "    NB_lya_position = NB_z(NNdata[:, -1].reshape(-1,))\n",
    "    for i, nb in enumerate(NB_lya_position):\n",
    "        NNdata[i, 2:55] = (\n",
    "            flux_to_mag(NNdata[i, 2:55], w_central[2:55])\n",
    "            - flux_to_mag(NNdata[i, 2:55][nb - 2], w_central[nb - 2])\n",
    "        )\n",
    "        NNdata[i, 55 : 55 + 4] = flux_to_mag(NNdata[i, 55 : 55 + 4], w_central[-4:])\n",
    "    NB_lya_position = NB_z(NNdata_test[:, -1].reshape(-1,))\n",
    "    for i, nb in enumerate(NB_lya_position - 2):\n",
    "        NNdata_test[i, :53] = (\n",
    "            flux_to_mag(NNdata_test[i, :53], w_central[:53])\n",
    "            - flux_to_mag(NNdata_test[i, :53][nb - 2], w_central[nb - 2])\n",
    "        )\n",
    "        NNdata_test[i, 53 : 53 + 4] = flux_to_mag(NNdata_test[i, 53 : 53 + 4], w_central[-4:])\n",
    "\n",
    "    NNdata = np.hstack(\n",
    "        [\n",
    "            NNdata[:, 2:55],\n",
    "            NNdata[:, 55 : 55 + 4],\n",
    "            NNdata[:, 55 + 4 + 2:]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(NNdata.shape)\n",
    "\n",
    "\n",
    "    N_sources_NN = NNdata.shape[0]\n",
    "\n",
    "    # Shuffle data\n",
    "    # shuffle_idx = np.random.permutation(np.arange(N_sources_NN))\n",
    "    shuffle_idx = np.arange(N_sources_NN)\n",
    "    NNdata = NNdata[shuffle_idx]\n",
    "\n",
    "    NNdata_L_input = NNdata_L_input[shuffle_idx].reshape(-1,)\n",
    "    NNdata_L_input[np.isnan(NNdata_L_input)] = 0\n",
    "    NNdata_is_qso = np.ones(N_sources_NN).astype(bool)\n",
    "    NNdata_is_qso[int(N_sources_NN / 2):] = False\n",
    "    NNdata_is_qso = NNdata_is_qso[shuffle_idx]\n",
    "\n",
    "    # Take logs\n",
    "    # NNdata[:, :53 + 4] = np.log10(NNdata[:, :53 + 4])\n",
    "    NNdata[np.isnan(NNdata)] = 99.\n",
    "    # NNdata[NNdata > 99.] = 99.\n",
    "\n",
    "    # NNdata_test[:, :53 + 4] = np.log10(NNdata_test[:, :53 + 4])\n",
    "    NNdata_test[np.isnan(NNdata_test)] = 99.\n",
    "    # NNdata_test[NNdata_test > 99.] = 99.\n",
    "\n",
    "    # Rescale data\n",
    "    mms = MinMaxScaler()\n",
    "    mms.fit(NNdata)\n",
    "    NNdata = mms.transform(NNdata)\n",
    "    NNdata_test = mms.transform(NNdata_test)\n",
    "    with open(f'MLmodels/RF{regname}_QSO-SF_scaler.sav', 'wb') as file:\n",
    "        pickle.dump(mms, file)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=0.95, svd_solver='full')\n",
    "\n",
    "    pca.fit(NNdata)\n",
    "    with open(f'MLmodels/RF{regname}_QSO-SF_pca.sav', 'wb') as file:\n",
    "        pickle.dump(pca, file)\n",
    "    NNdata = pca.transform(NNdata)\n",
    "    NNdata_test = pca.transform(NNdata_test)\n",
    "\n",
    "    NNdata_train = NNdata\n",
    "    NNlabels_train = NNdata_L_input\n",
    "\n",
    "    NNlabels_train = NNlabels_train.reshape(-1,)\n",
    "    NNlabels_test = NNlabels_test.reshape(-1,)\n",
    "\n",
    "    return NNdata_L_input, NNdata_train, NNdata_test, NNlabels_train, NNlabels_test, NNdata_L_Arr\n",
    "\n",
    "regname = 'mag15-23'\n",
    "NNdata_L_input, NNdata_train, NNdata_test, NNlabels_train, NNlabels_test, NNdata_L_Arr = prepare_dataset(regname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "(sys.getsizeof(NNdata_train)) / 1e6 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_grid_search():\n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'bootstrap': [False],\n",
    "        'max_depth': [50, 75, 100, 125, 150],\n",
    "        'max_features': [0.3],\n",
    "        'min_samples_leaf': [3, 4, 5, 6],\n",
    "        'min_samples_split': [4, 5, 6],\n",
    "        'n_estimators': [100]\n",
    "    }\n",
    "    # Create a based model\n",
    "    rf = RandomForestRegressor()\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf, param_grid=param_grid, \n",
    "        cv=KFold(5), n_jobs=10, pre_dispatch='2*n_jobs',\n",
    "        verbose=3\n",
    "    )\n",
    "\n",
    "    grid_search.fit(NNdata_train, NNlabels_train)\n",
    "\n",
    "    return grid_search.best_params_\n",
    "\n",
    "# best_params = do_grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The regressor\n",
    "best_params = {\n",
    "    'bootstrap': False,\n",
    "    'max_depth': 300,\n",
    "    'max_features': 0.3,\n",
    "    'min_samples_leaf': 6,\n",
    "    'min_samples_split': 5,\n",
    "    'n_estimators': 200,\n",
    "    'verbose': True,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "reg = RandomForestRegressor(**best_params)\n",
    "reg.set_params(n_estimators=400, n_jobs=-1)\n",
    "\n",
    "# Train it\n",
    "reg.fit(NNdata_test, NNlabels_test)\n",
    "\n",
    "# with open(f'MLmodels/RF{regname}_QSO-SF_regressor.sav', 'rb') as file:\n",
    "#     reg = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "# ax.plot(grid_search.cv_results_['mean_train_score'], marker='s')\n",
    "# ax.plot(grid_search.cv_results_['mean_test_score'], marker='s')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Arr_pred = reg.predict(NNdata_test)\n",
    "\n",
    "NNdata_test_raw = pd.read_csv(f'MLmodels/datasets/dataset_{regname}_test.csv').to_numpy()[:, 1:]\n",
    "\n",
    "test_mag = flux_to_mag(NNdata_test_raw[:, 56], 6750)\n",
    "test_mask = (test_mag < 25)\n",
    "\n",
    "print(f'Test score = {reg.score(NNdata_test[test_mask], NNlabels_test[test_mask])}')\n",
    "print(f'Train score = {reg.score(NNdata_train, NNlabels_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'MLmodels/RF{regname}_QSO-SF_regressor.sav', 'wb') as file:\n",
    "    pickle.dump(reg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(is_qso, maskk, title='', nb_c=-3):\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "    mask = is_qso & maskk\n",
    "    Z, x, y = np.histogram2d(\n",
    "        L_lya_test[mask].reshape(-1,), L_Arr_pred[mask],\n",
    "        bins=(np.linspace(41, 47, 30), np.linspace(41, 47, 30))\n",
    "    )\n",
    "\n",
    "    H_min = np.amin(Z)\n",
    "    H_max = np.amax(Z)\n",
    "\n",
    "    y_centers = 0.5 * (y[1:] + y[:-1])\n",
    "    x_centers = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "    N_bins = 10000\n",
    "\n",
    "    H_Arr = np.linspace(H_min, H_max, N_bins)[::-1]\n",
    "\n",
    "    fact_up_Arr = np.zeros(N_bins)\n",
    "\n",
    "    TOTAL_H = np.sum(Z)\n",
    "\n",
    "    for iii in range(0, N_bins):\n",
    "\n",
    "        mask = Z > H_Arr[iii]\n",
    "\n",
    "        fact_up_Arr[iii] = np.sum(Z[mask]) / TOTAL_H\n",
    "\n",
    "    H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1 sigma\n",
    "    H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2 sigma\n",
    "    H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 3 sigma\n",
    "\n",
    "    ax.contour(\n",
    "        x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "        colors='C0'\n",
    "    )\n",
    "\n",
    "    mask = ~is_qso & maskk\n",
    "    Z, x, y = np.histogram2d(\n",
    "        L_lya_test[mask].reshape(-1,), L_Arr_pred[mask],\n",
    "        bins=(np.linspace(41, 47, 30), np.linspace(41, 47, 30))\n",
    "    )\n",
    "\n",
    "    H_min = np.amin(Z)\n",
    "    H_max = np.amax(Z)\n",
    "\n",
    "    y_centers = 0.5 * (y[1:] + y[:-1])\n",
    "    x_centers = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "    N_bins = 10000\n",
    "\n",
    "    H_Arr = np.linspace(H_min , H_max , N_bins )[::-1]\n",
    "\n",
    "    fact_up_Arr = np.zeros(N_bins)\n",
    "\n",
    "    TOTAL_H = np.sum(Z)\n",
    "\n",
    "    for iii in range(0, N_bins):\n",
    "\n",
    "        mask = Z > H_Arr[iii]\n",
    "\n",
    "        fact_up_Arr[iii] = np.sum(Z[ mask ]) / TOTAL_H\n",
    "\n",
    "    H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1sigma\n",
    "    H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2sigma\n",
    "    H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 2sigma\n",
    "\n",
    "    ax.contour(\n",
    "        x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "        colors='C1'\n",
    "    )\n",
    "\n",
    "    mask = (is_qso | ~is_qso) & maskk\n",
    "    Z, x, y = np.histogram2d(\n",
    "        L_lya_test[mask].reshape(-1,), NNdata_L_Arr[mask],\n",
    "        bins=(np.linspace(41, 47, 30), np.linspace(41, 47, 30))\n",
    "    )\n",
    "\n",
    "    H_min = np.amin(Z)\n",
    "    H_max = np.amax(Z)\n",
    "\n",
    "    y_centers = 0.5 * (y[1:] + y[:-1])\n",
    "    x_centers = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "    N_bins = 10000\n",
    "\n",
    "    H_Arr = np.linspace(H_min, H_max, N_bins)[::-1]\n",
    "\n",
    "    fact_up_Arr = np.zeros(N_bins)\n",
    "\n",
    "    TOTAL_H = np.sum(Z)\n",
    "\n",
    "    for iii in range(0, N_bins):\n",
    "\n",
    "        mask = Z > H_Arr[iii]\n",
    "\n",
    "        fact_up_Arr[iii] = np.sum(Z[mask]) / TOTAL_H\n",
    "\n",
    "    H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1 sigma\n",
    "    H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2 sigma\n",
    "    H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 3 sigma\n",
    "\n",
    "    ax.contour(\n",
    "        x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "        colors='k', linestyles='--'\n",
    "    )\n",
    "\n",
    "    x = np.linspace(40, 48, 100)\n",
    "    ax.plot(x, x, linestyle='--', color='red', label='1:1')\n",
    "\n",
    "    ax.set_ylabel('Retrieved $\\log L$', fontsize=15)\n",
    "    ax.set_xlabel('Real $\\log L$', fontsize=15)\n",
    "\n",
    "    ax.set_ylim((41, 47))\n",
    "    ax.set_xlim((41, 47))\n",
    "\n",
    "    if len(title) > 0:\n",
    "        ax.set_title(title, fontsize=20)\n",
    "\n",
    "    # Detec lim\n",
    "\n",
    "    detec_lim = np.vstack(\n",
    "        (\n",
    "            pd.read_csv('csv/5sigma_depths_NB.csv', header=None),\n",
    "            pd.read_csv('csv/5sigma_depths_BB.csv', header=None)\n",
    "        )\n",
    "    )[:, 1]\n",
    "\n",
    "    flambda_lim = mag_to_flux(detec_lim[nb_c], w_central[nb_c]) * 3\n",
    "\n",
    "    ew0_lim = 20 # A\n",
    "    z = w_central[nb_c] / 1215.67 - 1\n",
    "    Fline_lim = ew0_lim * flambda_lim * (1 + z)\n",
    "    dL = cosmo.luminosity_distance(z).to(u.cm).value\n",
    "    L_lim = np.log10(Fline_lim * 4*np.pi * dL**2)\n",
    "\n",
    "    ax.axhline(L_lim, ls='--', color='green', label='L limit')\n",
    "\n",
    "    ax.legend(fontsize=15)\n",
    "    # plt.savefig(f'/home/alberto/Desktop/{title}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_qso_test = np.ones(len(NNlabels_test)).astype(bool)\n",
    "is_qso_test[int(len(is_qso_test) / 2) :] = False\n",
    "\n",
    "L_lya_test = NNlabels_test\n",
    "\n",
    "NB_lya_position = NB_z(NNdata_test_raw[:, -1].reshape(-1,))\n",
    "nb_c = 7\n",
    "ftags = load_filter_tags()\n",
    "maskk = (NB_lya_position == nb_c)\n",
    "plot_contours(is_qso_test, maskk, title=f'{regname}, {ftags[nb_c]}', nb_c=nb_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(NNdata_test) // 5\n",
    "train_sizes = np.array([batch_size * (j + 1) for j in range(5)]).astype(int)\n",
    "train_score = np.empty(5)\n",
    "test_score = np.empty(5)\n",
    "\n",
    "reg.set_params(n_jobs=-1)\n",
    "\n",
    "for k in range(5):\n",
    "    reg.fit(\n",
    "        NNdata_test[: (k + 1) * (batch_size)],\n",
    "        NNlabels_test[:(k + 1) * (batch_size)]\n",
    "    )\n",
    "\n",
    "    train_score[k] = reg.score(\n",
    "        NNdata_test[: (k + 1) * (batch_size)],\n",
    "        NNlabels_test[:(k + 1) * (batch_size)]\n",
    "    )\n",
    "    test_score[k] = reg.score(\n",
    "        NNdata_train,\n",
    "        NNlabels_train\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "x_ticks = train_sizes\n",
    "ax.plot(x_ticks, train_score, marker='s', label='Training score')\n",
    "ax.plot(x_ticks, test_score, marker='s', label='Test score')\n",
    "\n",
    "ax.legend(fontsize=15)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=15)\n",
    "ax.set_xlabel('Training set fraction', fontsize=15)\n",
    "ax.set_title(f'Random Forest, {regname}', fontsize=20)\n",
    "\n",
    "ax.tick_params(labelsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = reg.feature_importances_\n",
    "np.flip(np.argsort(a))[:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'MLmodels/RF{regname}_QSO-SF_pca.sav', 'rb') as file:\n",
    "    pca = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ade4bca3e0042e0da78fecdb82351169c0f2ccedb06a0d7cf7342df8f7e47af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
