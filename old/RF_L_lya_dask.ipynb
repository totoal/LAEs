{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "\n",
    "import pickle\n",
    "\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.preprocessing import MinMaxScaler\n",
    "from dask_ml.decomposition import IncrementalPCA\n",
    "from dask.distributed import Client\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_central = central_wavelength()\n",
    "nb_fwhm_Arr = nb_fwhm(range(60))\n",
    "w_lya = 1215.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The initial features are:\n",
    "- The fluxes of the first 55 NBs\n",
    "- The errors of the first 55 NBs\n",
    "- 4 BB fluxes\n",
    "- 4 BB errors\n",
    "- The estimated L\n",
    "- The estimated z\n",
    "TOTAL = 120 features\n",
    "(PCA to be applied below)\n",
    "'''\n",
    "\n",
    "# The data set is the nice_lya sample\n",
    "NNdata = dd.read_csv(\n",
    "    'MLmodels/datasets/dataset_mag0_2_000_000.csv',\n",
    "    usecols=range(1, 121)\n",
    ").to_dask_array(lengths=True)\n",
    "NNdata_L_input = dd.read_csv(\n",
    "    'MLmodels/datasets/tags_mag0_2_000_000.csv',\n",
    "    usecols=[1]\n",
    ").to_dask_array(lengths=True)\n",
    "\n",
    "# Rechunk\n",
    "chunksize = '200 MiB'\n",
    "NNdata = da.rechunk(NNdata, chunks=chunksize)\n",
    "NNdata_L_input = da.rechunk(NNdata_L_input, chunks=chunksize)\n",
    "\n",
    "N_sources_NN = NNdata.shape[0]\n",
    "\n",
    "is_qso = da.ones(N_sources_NN, chunks=chunksize).astype(bool)\n",
    "\n",
    "# Shuffle data\n",
    "shuffle_idx = da.random.permutation(da.arange(N_sources_NN))\n",
    "NNdata = da.rechunk(NNdata[shuffle_idx], chunks=chunksize)\n",
    "\n",
    "NNdata_L_input = NNdata_L_input[shuffle_idx].reshape(-1,)\n",
    "NNdata_L_input[da.isnan(NNdata_L_input)] = 0\n",
    "NNdata_is_qso = da.ones(N_sources_NN).astype(bool)\n",
    "NNdata_is_qso[int(N_sources_NN / 2):] = False\n",
    "NNdata_is_qso = NNdata_is_qso[shuffle_idx]\n",
    "\n",
    "# Take logs\n",
    "NNdata[:, :55 + 4] = da.log10(NNdata[:, :55 + 4])\n",
    "\n",
    "NNdata[da.isnan(NNdata)] = -99.\n",
    "NNdata[NNdata > 99.] = 99.\n",
    "\n",
    "# Rechunk\n",
    "NNdata = da.rechunk(NNdata, chunks=chunksize)\n",
    "NNdata_L_input = da.rechunk(NNdata_L_input, chunks=chunksize)\n",
    "\n",
    "# Rescale data\n",
    "client = Client(processes=False)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "with joblib.parallel_backend('dask'):\n",
    "    mms.fit(NNdata)\n",
    "NNdata = mms.transform(NNdata)\n",
    "with open('MLmodels/RFmag0_QSO-SF_scaler.sav', 'wb') as file:\n",
    "    pickle.dump(mms, file)\n",
    "\n",
    "# Rechunk\n",
    "NNdata = da.rechunk(NNdata, chunks=[1, NNdata.shape[1]])\n",
    "NNdata_L_input = da.rechunk(NNdata_L_input, chunks=chunksize)\n",
    "\n",
    "# Apply PCA\n",
    "pca = IncrementalPCA(n_components=60, svd_solver='auto')\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    pca.fit(NNdata)\n",
    "with open('MLmodels/RFmag0_QSO-SF_pca.sav', 'wb') as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "NNdata = pca.transform(NNdata)\n",
    "\n",
    "# Rechunk\n",
    "NNdata = da.rechunk(NNdata, chunks=chunksize)\n",
    "NNdata_L_input = da.rechunk(NNdata_L_input, chunks=chunksize)\n",
    "\n",
    "# Split dataset\n",
    "NNdata_train, NNdata_test, NNlabels_train, NNlabels_test =\\\n",
    "    train_test_split(NNdata, NNdata_L_input, test_size=0.2, shuffle=False)\n",
    "\n",
    "NNlabels_train = NNlabels_train.reshape(-1,)\n",
    "NNlabels_test = NNlabels_test.reshape(-1,)\n",
    "\n",
    "N_train = len(NNlabels_train)\n",
    "N_test = len(NNlabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set is the nice_lya sample\n",
    "NNdata = dd.read_csv(\n",
    "    'MLmodels/datasets/dataset_mag0_2_000_000.csv',\n",
    "    usecols=range(1, 121)\n",
    ").to_dask_array(lengths=True)\n",
    "NNdata_L_input = dd.read_csv(\n",
    "    'MLmodels/datasets/tags_mag0_2_000_000.csv',\n",
    "    usecols=[1]\n",
    ").to_dask_array(lengths=True)\n",
    "\n",
    "# Rechunk\n",
    "chunksize = '200 MiB'\n",
    "NNdata = da.rechunk(NNdata, chunks=chunksize)\n",
    "NNdata_L_input = da.rechunk(NNdata_L_input, chunks=chunksize)\n",
    "\n",
    "N_sources_NN = NNdata.shape[0]\n",
    "\n",
    "is_qso = da.ones(N_sources_NN, chunks=chunksize).astype(bool)\n",
    "\n",
    "# Shuffle data\n",
    "shuffle_idx = da.random.permutation(da.arange(N_sources_NN))\n",
    "NNdata = da.rechunk(NNdata[shuffle_idx], chunks=chunksize)\n",
    "\n",
    "NNdata_L_input = NNdata_L_input[shuffle_idx].reshape(-1,)\n",
    "NNdata_L_input[da.isnan(NNdata_L_input)] = 0\n",
    "NNdata_is_qso = da.ones(N_sources_NN).astype(bool)\n",
    "NNdata_is_qso[int(N_sources_NN / 2):] = False\n",
    "NNdata_is_qso = NNdata_is_qso[shuffle_idx]\n",
    "\n",
    "# Take logs\n",
    "NNdata[:, :55 + 4] = da.log10(NNdata[:, :55 + 4])\n",
    "\n",
    "NNdata[da.isnan(NNdata)] = -99.\n",
    "NNdata[NNdata > 99.] = 99.\n",
    "\n",
    "# Rechunk\n",
    "NNdata = da.rechunk(NNdata, chunks=chunksize)\n",
    "NNdata_L_input = da.rechunk(NNdata_L_input, chunks=chunksize)\n",
    "\n",
    "# Rescale data\n",
    "client = Client(processes=False)\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "with joblib.parallel_backend('dask'):\n",
    "    mms.fit(NNdata)\n",
    "NNdata = mms.transform(NNdata)\n",
    "with open('MLmodels/RFmag0_QSO-SF_scaler.sav', 'wb') as file:\n",
    "    pickle.dump(mms, file)\n",
    "\n",
    "# Rechunk\n",
    "NNdata = da.rechunk(NNdata, chunks=NNdata.shape[0] / 5)\n",
    "NNdata_L_input = da.rechunk(NNdata_L_input, chunks=chunksize)\n",
    "\n",
    "# Apply PCA\n",
    "pca = IncrementalPCA(n_components=60, svd_solver='auto')\n",
    "\n",
    "print('Lets fitttttt')\n",
    "with joblib.parallel_backend('dask'):\n",
    "    pca.fit(NNdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = IncrementalPCA(n_components=60, svd_solver='full')\n",
    "\n",
    "print('Lets fitttttt')\n",
    "with joblib.parallel_backend('dask'):\n",
    "    pca.fit(NNdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "# param_grid = {\n",
    "#     'bootstrap': [False],\n",
    "#     'max_depth': [100, 200, 300],\n",
    "#     'max_features': [0.3],\n",
    "#     'min_samples_leaf': [2],\n",
    "#     'min_samples_split': [4],\n",
    "#     'n_estimators': [150]\n",
    "# }\n",
    "# # Create a based model\n",
    "# rf = RandomForestRegressor()\n",
    "# # Instantiate the grid search model\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=rf, param_grid=param_grid, \n",
    "#     cv=5, n_jobs=15, verbose=3, return_train_score=True\n",
    "# )\n",
    "\n",
    "# grid_search.fit(NNdata_train, NNlabels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_lya_test = NNlabels_test\n",
    "\n",
    "# The regressor\n",
    "best_params = {\n",
    "    'bootstrap': False,\n",
    "    'max_depth': 100,\n",
    "    'max_features': 0.3,\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4,\n",
    "    'n_estimators': 200,\n",
    "    'verbose': True,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "# best_params = grid_search.best_params_\n",
    "reg = RandomForestRegressor(**best_params)\n",
    "reg.set_params(n_estimators=200)\n",
    "\n",
    "# Train it\n",
    "with joblib.parallel_backend('dask'):\n",
    "    reg.fit(NNdata_train, NNlabels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "# ax.plot(grid_search.cv_results_['mean_train_score'], marker='s')\n",
    "# ax.plot(grid_search.cv_results_['mean_test_score'], marker='s')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Arr_pred = reg.predict(NNdata_test)\n",
    "print(f'Test score = {reg.score(NNdata_test, NNlabels_test)}')\n",
    "print(f'Train score = {reg.score(NNdata_train, NNlabels_train)}')\n",
    "\n",
    "L_Arr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MLmodels/RFmag0_QSO-SF_regressor.sav', 'wb') as file:\n",
    "    pickle.dump(reg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(is_qso, title='', nb_c=-3):\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "    mask = is_qso\n",
    "    Z, x, y = np.histogram2d(\n",
    "        L_lya_test[mask].reshape(-1,), L_Arr_pred[mask],\n",
    "        bins=(np.linspace(41, 47, 30), np.linspace(41, 47, 30))\n",
    "    )\n",
    "\n",
    "    H_min = np.amin(Z)\n",
    "    H_max = np.amax(Z)\n",
    "\n",
    "    y_centers = 0.5 * (y[1:] + y[:-1])\n",
    "    x_centers = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "    N_bins = 10000\n",
    "\n",
    "    H_Arr = np.linspace(H_min, H_max, N_bins)[::-1]\n",
    "\n",
    "    fact_up_Arr = np.zeros(N_bins)\n",
    "\n",
    "    TOTAL_H = np.sum(Z)\n",
    "\n",
    "    for iii in range(0, N_bins):\n",
    "\n",
    "        mask = Z > H_Arr[iii]\n",
    "\n",
    "        fact_up_Arr[iii] = np.sum(Z[mask]) / TOTAL_H\n",
    "\n",
    "    H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1 sigma\n",
    "    H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2 sigma\n",
    "    H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 3 sigma\n",
    "\n",
    "    ax.contour(\n",
    "        x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "        colors='C0'\n",
    "    )\n",
    "\n",
    "    mask = ~is_qso\n",
    "    Z, x, y = np.histogram2d(\n",
    "        L_lya_test[mask].reshape(-1,), L_Arr_pred[mask],\n",
    "        bins=(np.linspace(41, 47, 30), np.linspace(41, 47, 30))\n",
    "    )\n",
    "\n",
    "    H_min = np.amin(Z)\n",
    "    H_max = np.amax(Z)\n",
    "\n",
    "    y_centers = 0.5 * (y[1:] + y[:-1])\n",
    "    x_centers = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "    N_bins = 10000\n",
    "\n",
    "    H_Arr = np.linspace(H_min , H_max , N_bins )[::-1]\n",
    "\n",
    "    fact_up_Arr = np.zeros(N_bins)\n",
    "\n",
    "    TOTAL_H = np.sum(Z)\n",
    "\n",
    "    for iii in range(0, N_bins):\n",
    "\n",
    "        mask = Z > H_Arr[iii]\n",
    "\n",
    "        fact_up_Arr[iii] = np.sum(Z[ mask ]) / TOTAL_H\n",
    "\n",
    "    H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1sigma\n",
    "    H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2sigma\n",
    "    H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 2sigma\n",
    "\n",
    "    ax.contour(\n",
    "        x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "        colors='C1'\n",
    "    )\n",
    "\n",
    "    x = np.linspace(40, 48, 100)\n",
    "    ax.plot(x, x, linestyle='--', color='red', label='1:1')\n",
    "\n",
    "    ax.set_ylabel('Retrieved $\\log L$', fontsize=15)\n",
    "    ax.set_xlabel('Real $\\log L$', fontsize=15)\n",
    "\n",
    "    ax.set_ylim((41, 47))\n",
    "    ax.set_xlim((41, 47))\n",
    "\n",
    "    if len(title) > 0:\n",
    "        ax.set_title(title, fontsize=20)\n",
    "\n",
    "    # Detec lim\n",
    "\n",
    "    detec_lim = np.vstack(\n",
    "        (\n",
    "            pd.read_csv('csv/5sigma_depths_NB.csv', header=None),\n",
    "            pd.read_csv('csv/5sigma_depths_BB.csv', header=None)\n",
    "        )\n",
    "    )[:, 1]\n",
    "\n",
    "    flambda_lim = mag_to_flux(detec_lim[nb_c], w_central[nb_c]) * 3\n",
    "\n",
    "    ew0_lim = 20 # A\n",
    "    z = w_central[nb_c] / 1215.67 - 1\n",
    "    Fline_lim = ew0_lim * flambda_lim * (1 + z)\n",
    "    dL = cosmo.luminosity_distance(z).to(u.cm).value\n",
    "    L_lim = np.log10(Fline_lim * 4*np.pi * dL**2)\n",
    "\n",
    "    ax.axhline(L_lim, ls='--', color='green', label='L limit')\n",
    "\n",
    "    ax.legend(fontsize=15)\n",
    "    # plt.savefig(f'/home/alberto/Desktop/{title}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_lya_test = np.array(L_lya_test)\n",
    "plot_contours(NNdata_is_qso[-int(N_sources_NN / 5):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.set_params(verbose=1)\n",
    "a = learning_curve(reg, NNdata, NNdata_L_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "x_ticks = a[0]\n",
    "ax.plot(x_ticks, a[1].sum(axis=1)/len(x_ticks), marker='s', label='Training score')\n",
    "ax.plot(x_ticks, a[2].sum(axis=1)/len(x_ticks), marker='s', label='Test score')\n",
    "\n",
    "ax.legend(fontsize=15)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=15)\n",
    "ax.set_xlabel('Training set fraction', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ade4bca3e0042e0da78fecdb82351169c0f2ccedb06a0d7cf7342df8f7e47af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
