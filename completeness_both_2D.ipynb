{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from my_functions import *\n",
    "from LF_puricomp_corrections import weights_LF, puricomp2d_weights\n",
    "\n",
    "import glob\n",
    "\n",
    "from scipy.integrate import simpson\n",
    "from scipy.stats import binned_statistic\n",
    "\n",
    "from astropy.cosmology import Planck18 as cosmo\n",
    "import astropy.units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_central = central_wavelength()\n",
    "nb_fwhm_Arr = nb_fwhm(range(60))\n",
    "w_lya = 1215.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load QSO catalog\n",
    "filename = ('/home/alberto/cosmos/JPAS_mocks_sep2021/'\n",
    "    'JPAS_mocks_classification_01sep_model11/Fluxes/Qso_jpas_mock_flam_train.cat')\n",
    "\n",
    "my_filter_order = np.arange(60)\n",
    "my_filter_order[[-4, -3, -2, -1]] = np.array([1, 12, 28, 43])\n",
    "my_filter_order[1:-4] += 1\n",
    "my_filter_order[12:-4] += 1\n",
    "my_filter_order[28:-4] += 1\n",
    "my_filter_order[43:-4] += 1\n",
    "\n",
    "qso_flx = pd.read_csv(\n",
    "    filename, sep=' ', usecols=range(2, 2 + 60)\n",
    ").to_numpy().T[my_filter_order]\n",
    "qso_err = pd.read_csv(\n",
    "    filename, sep=' ', usecols=range(2 + 60, 2 + 60 + 60)\n",
    ").to_numpy().T[my_filter_order]\n",
    "qso_zspec = pd.read_csv(filename, sep=' ', usecols=[127]).to_numpy().reshape(-1, )\n",
    "\n",
    "# Randomly sample sources corresponding to 200 deg2\n",
    "# idx = np.random.randint(0, 100000, 510 * 200)\n",
    "idx = np.arange(100_000)\n",
    "qso_flx = qso_flx[:, idx]\n",
    "qso_err = qso_err[:, idx]\n",
    "qso_zspec = qso_zspec[idx]\n",
    "\n",
    "Lya_fts = pd.read_csv('csv/Lya_fts.csv')\n",
    "EW_qso = np.abs(Lya_fts.LyaEW)[idx] / (qso_zspec + 1)\n",
    "\n",
    "# Apply errors\n",
    "np.random.seed(22)\n",
    "# qso_flx += qso_err * np.random.normal(size=qso_err.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load SF catalog\n",
    "\n",
    "filename = '/home/alberto/almacen/Source_cats/LAE_10deg_z2-5/'\n",
    "files = glob.glob(filename +'data*')\n",
    "files.sort()\n",
    "fi = []\n",
    "\n",
    "for name in files:\n",
    "    fi.append(pd.read_csv(name))\n",
    "\n",
    "data = pd.concat(fi, axis=0, ignore_index=True)\n",
    "\n",
    "sf_flx = data.to_numpy()[:, 1 : 60 + 1].T\n",
    "sf_err = data.to_numpy()[:, 60 + 1 : 120 + 1].T\n",
    "\n",
    "sf_flx += np.random.normal(size=(sf_err.shape)) * sf_err\n",
    "\n",
    "# files2 = []\n",
    "# files3 = []\n",
    "# for i in range(len(files)):\n",
    "#     files2.append(f'{filename}SEDs{i + 1}.csv')\n",
    "#     files2.sort()\n",
    "#     files3.append(f'{filename}SEDs_no_line{i + 1}.csv')\n",
    "#     files3.sort()\n",
    "# fi = []\n",
    "# for name in files2:\n",
    "#     fi.append(pd.read_csv(name, header=None))\n",
    "# fi3 = []\n",
    "# for name in files3:\n",
    "#     fi3.append(pd.read_csv(name, header=None))\n",
    "\n",
    "# mock = {}\n",
    "# mock['SEDs'] = pd.concat(fi, axis=0, ignore_index=True).to_numpy()\n",
    "# mock['SEDs_no_line'] = pd.concat(fi, axis=0, ignore_index=True).to_numpy()\n",
    "# mock['w_Arr'] = np.load(filename + 'w_Arr.npy')\n",
    "\n",
    "EW_sf = data['EW0'].to_numpy()\n",
    "sf_zspec = data['z'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_flx = np.hstack((qso_flx, sf_flx))\n",
    "pm_err = np.hstack((qso_err, sf_err))\n",
    "zspec = np.concatenate((qso_zspec, sf_zspec))\n",
    "EW_lya = np.concatenate((EW_qso, EW_sf))\n",
    "\n",
    "N_sf = sf_flx.shape[1]\n",
    "N_qso = qso_flx.shape[1]\n",
    "\n",
    "qso_dL = cosmo.luminosity_distance(qso_zspec).to(u.cm).value\n",
    "sf_dL = cosmo.luminosity_distance(sf_zspec).to(u.cm).value\n",
    "\n",
    "sf_L = data['L_lya'].to_numpy()\n",
    "\n",
    "sf_flambda = 10 ** sf_L / (4*np.pi * sf_dL **2)\n",
    "qso_flambda = Lya_fts.LyaF * 1e-17\n",
    "\n",
    "qso_L = np.log10(qso_flambda * 4*np.pi * qso_dL ** 2)\n",
    "\n",
    "L_lya = np.concatenate((qso_L, sf_L))\n",
    "fline = np.concatenate((qso_flambda, sf_flambda))\n",
    "\n",
    "is_qso = np.concatenate((np.ones(N_qso), np.zeros(N_sf))).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel sf_flx\n",
    "%xdel sf_err\n",
    "%xdel qso_flx\n",
    "%xdel qso_err\n",
    "%xdel sf_zspec\n",
    "%xdel qso_zspec\n",
    "%xdel EW_sf\n",
    "%xdel EW_qso\n",
    "%xdel qso_dL\n",
    "%xdel sf_L\n",
    "%xdel qso_L\n",
    "%xdel sf_flambda\n",
    "%xdel qso_flambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lya = 1215.67 # A\n",
    "N_sources = pm_flx.shape[1]\n",
    "N_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag = flux_to_mag(pm_flx[-2], w_central[-2])\n",
    "mag[np.isnan(mag)] = 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lya search\n",
    "cont_est_lya, cont_err_lya = estimate_continuum(pm_flx, pm_err, IGM_T_correct=True)\n",
    "line = is_there_line(pm_flx, pm_err, cont_est_lya, cont_err_lya, 20)\n",
    "lya_lines, lya_cont_lines = identify_lines(line, pm_flx, pm_err, first=True)\n",
    "lya_lines = np.array(lya_lines)\n",
    "\n",
    "# Other lines\n",
    "cont_est_other, cont_err_other = estimate_continuum(pm_flx, pm_err, IGM_T_correct=False)\n",
    "line_other = is_there_line(pm_flx, pm_err, cont_est_other, cont_err_other,\n",
    "    400, obs=True)\n",
    "other_lines = identify_lines(line_other, pm_flx, pm_err)\n",
    "\n",
    "# Compute z\n",
    "z_Arr = np.zeros(N_sources)\n",
    "z_Arr[np.where(np.array(lya_lines) != -1)] =\\\n",
    "    z_NB(np.array(lya_cont_lines)[np.where(np.array(lya_lines) != -1)])\n",
    "\n",
    "nice_z = np.abs(z_Arr - zspec) < 0.12\n",
    "\n",
    "%xdel cont_est_other\n",
    "%xdel cont_err_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_min = 17\n",
    "mag_max = 24\n",
    "\n",
    "nb_min = 5\n",
    "nb_max = 20\n",
    "\n",
    "nbs_to_consider = np.arange(nb_min, nb_max + 1)\n",
    "\n",
    "nb_cut = (np.array(lya_lines) >= nb_min) & (np.array(lya_lines) <= nb_max)\n",
    "\n",
    "z_min = (w_central[nb_min] - nb_fwhm_Arr[nb_min] * 0.5) / w_lya - 1\n",
    "z_max = (w_central[nb_max] + nb_fwhm_Arr[nb_max] * 0.5) / w_lya - 1\n",
    "\n",
    "z_cut = (z_min < z_Arr) & (z_Arr < z_max)\n",
    "zspec_cut = (z_min < zspec) & (zspec < z_max)\n",
    "ew_cut = EW_lya > 20\n",
    "mag_cut = (mag > mag_min) & (mag < mag_max)\n",
    "\n",
    "nice_lya = nice_lya_select(\n",
    "    lya_lines, other_lines, pm_flx, pm_err, cont_est_lya, z_Arr\n",
    ")\n",
    "nice_lya = nice_lya & z_cut & mag_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fractions QSO / SF\n",
    "\n",
    "good_qso = len(np.where(np.where(nice_lya & nice_z)[0] < N_qso)[0])\n",
    "bad_qso = len(np.where(np.where(nice_lya & ~nice_z)[0] < N_qso)[0])\n",
    "N_sel = count_true(nice_lya)\n",
    "N_sel_good = count_true(nice_lya & nice_z)\n",
    "\n",
    "\n",
    "good_frac = good_qso / N_sel_good\n",
    "bad_frac = bad_qso / (N_sel - N_sel_good)\n",
    "\n",
    "purity = count_true(nice_z & nice_lya) / count_true(nice_lya)\n",
    "\n",
    "print(f'Good QSOs: {good_qso} | Bad QSOs: {bad_qso}')\n",
    "print(f'Good SFs: {N_sel_good - good_qso} | Bad SFs: {N_sel - N_sel_good - bad_qso}')\n",
    "print()\n",
    "print('Good frac: {0:0.2f}'.format(good_frac))\n",
    "print('Bad frac: {0:0.2f}'.format(bad_frac))\n",
    "print()\n",
    "print('Purity = {0:0.2f}'.format(purity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EW_err(fnb, fnb_err, fcont, fcont_err, z, z_err, fwhm):\n",
    "    e1 = fnb_err * fwhm / fcont / (1 + z)\n",
    "    e2 = fcont_err * fwhm / (-fcont ** -2 * (1 + z))\n",
    "    e3 = z_err * fwhm * (fnb - fcont) / fcont * (-1) / ((1 + z) ** 2)\n",
    "\n",
    "    return (e1**2 + e2**2 + e3**2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EW_nb_Arr, EW_nb_e, L_Arr, L_e_Arr, flambda, flambda_e = EW_L_NB(\n",
    "    pm_flx, pm_err, cont_est_lya, cont_err_lya, z_Arr, lya_lines\n",
    ")\n",
    "\n",
    "def compute_corrections(flambda, fline, L_Arr):\n",
    "    F_cor = np.ones(60)\n",
    "    L_nb_err = np.ones(60)\n",
    "\n",
    "    for nb in nbs_to_consider:\n",
    "        to_cor = nice_z & (lya_lines == nb) & mag_cut\n",
    "        F_to_cor = (flambda / fline)[to_cor]\n",
    "        F_to_cor[np.isinf(F_to_cor)] = np.nan\n",
    "        F_cor[nb] = np.nanmedian(F_to_cor)\n",
    "\n",
    "        L_to_cor = L_Arr[to_cor] - L_lya[to_cor]\n",
    "        \n",
    "        L_percentiles = np.nanpercentile(L_to_cor, [16, 50, 84])\n",
    "        L_nb_err[nb] = (L_percentiles[2] - L_percentiles[1])\n",
    "\n",
    "    return F_cor, L_nb_err\n",
    "\n",
    "F_cor, L_nb_err = compute_corrections(flambda, fline, L_Arr)\n",
    "\n",
    "np.save('npy/L_nb_err.npy', L_nb_err)\n",
    "\n",
    "EW_nb_Arr, EW_nb_e, L_Arr, L_e_Arr, flambda, flambda_e = EW_L_NB(\n",
    "    pm_flx, pm_err, cont_est_lya, cont_err_lya, z_Arr, lya_lines,\n",
    "    F_bias=F_cor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_L_Lbin_err(L_Arr, L_lya, L_binning):\n",
    "    L_Lbin_err_plus = np.ones(len(L_binning) - 1) * 99\n",
    "    L_Lbin_err_minus = np.ones(len(L_binning) - 1) * 99\n",
    "    last = [99., 99.]\n",
    "    for i in range(len(L_binning) - 1):\n",
    "        in_bin = (L_Arr >= L_binning[i]) & (L_Arr < L_binning[i + 1])\n",
    "        if count_true(in_bin) == 0:\n",
    "            L_Lbin_err_plus[i] = last[0]\n",
    "            L_Lbin_err_minus[i] = last[1]\n",
    "            continue\n",
    "        perc = np.percentile((L_Arr - L_lya)[in_bin], [16, 50, 84])\n",
    "        L_Lbin_err_plus[i] = perc[2] - perc[1]\n",
    "        L_Lbin_err_minus[i] = perc[1] - perc[0]\n",
    "        \n",
    "        last = [L_Lbin_err_plus[i], L_Lbin_err_minus[i]]\n",
    "\n",
    "    return np.vstack((L_Lbin_err_plus, L_Lbin_err_minus))\n",
    "\n",
    "L_binning = np.linspace(42, 46, 15 + 1)\n",
    "L_Lbin_err = compute_L_Lbin_err(L_Arr[nice_z], L_lya[nice_z], L_binning)\n",
    "np.save('npy/L_nb_err.npy', L_Lbin_err)\n",
    "np.save('npy/L_nb_err_binning.npy', L_binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Lbin_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel pm_flx\n",
    "%xdel pm_err\n",
    "%xdel cont_est_lya\n",
    "%xdel cont_err_lya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, src in enumerate(np.where(~nice_lya & (L_lya > 45) & zspec_cut & mag_cut & z_cut)[0]):\n",
    "#     if k == 15: break\n",
    "#     fig = plt.figure(figsize=(8, 6))\n",
    "#     ax = plot_JPAS_source(pm_flx[:, src], pm_err[:, src])\n",
    "#     print(f'z_NB = {z_Arr[src]}')\n",
    "#     print(f'zspec = {zspec[src]}')\n",
    "#     print(bl[src])\n",
    "    \n",
    "#     ax.axvline(w_central[lya_lines[src]], label='Selected NB')\n",
    "#     ax.axvline(w_lya * (1 + zspec[src]), ls='--', c='r')\n",
    "#     ax.legend(fontsize=13)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "# bins = np.linspace(18, 26, 20)\n",
    "\n",
    "# ax.hist(mag[nice_lya & z_cut & nice_z], label='Good z', histtype='step', bins=bins)\n",
    "# ax.hist(mag[nice_lya & z_cut & ~nice_z], label='Bad z', histtype='step', bins=bins)\n",
    "\n",
    "# ax.legend(fontsize=15, loc=2)\n",
    "\n",
    "# ax.set_xlabel('r', fontsize=15)\n",
    "# ax.set_ylabel('N', fontsize=15)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# ####\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "# L_bins = np.linspace(42.5, 46, 30)\n",
    "\n",
    "# goodh = L_Arr[nice_lya & z_cut & nice_z]\n",
    "# badh = L_Arr[nice_lya & z_cut & ~nice_z]\n",
    "\n",
    "# ax.hist(goodh, label='Good z', histtype='step', bins=L_bins)\n",
    "# ax.hist(badh, label='Bad z', histtype='step', bins=L_bins)\n",
    "\n",
    "# ax.legend(fontsize=15)\n",
    "\n",
    "# ax.set_xlabel('log L$_\\mathrm{line}$ retrieved', fontsize=15)\n",
    "# ax.set_ylabel('N', fontsize=15)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# ####\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "# L_bins = np.linspace(42.5, 46, 30)\n",
    "\n",
    "# goodh = L_Arr[nice_lya & z_cut & nice_z]\n",
    "# goodh_real = L_lya[nice_lya & z_cut & nice_z]\n",
    "# allh_real = L_lya[zspec_cut & ew_cut & mag_cut]\n",
    "\n",
    "# ax.hist(goodh, label='Calc', histtype='step', bins=L_bins)\n",
    "# ax.hist(goodh_real, label='Real', histtype='step', bins=L_bins)\n",
    "# # ax.hist(allh_real, label='All', histtype='step', bins=L_bins)\n",
    "\n",
    "# ax.legend(fontsize=15)\n",
    "\n",
    "# ax.set_xlabel('log L$_\\mathrm{line}$ retrieved', fontsize=15)\n",
    "# ax.set_ylabel('N', fontsize=15)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "Z, x, y = np.histogram2d(\n",
    "    L_lya[is_qso & nice_lya & nice_z], L_Arr[is_qso & nice_lya & nice_z],\n",
    "    bins=(np.linspace(42, 47, 30), np.linspace(42, 47, 30))\n",
    ")\n",
    "\n",
    "H_min = np.amin(Z)\n",
    "H_max = np.amax(Z)\n",
    "\n",
    "y_centers = 0.5 * ( y[1:] + y[:-1] )\n",
    "x_centers = 0.5 * ( x[1:] + x[:-1] )\n",
    "\n",
    "N_bins = 10000\n",
    "\n",
    "H_Arr = np.linspace( H_min , H_max , N_bins )[::-1]\n",
    "\n",
    "fact_up_Arr = np.zeros( N_bins )\n",
    "\n",
    "TOTAL_H = np.sum(Z)\n",
    "\n",
    "for iii in range(0, N_bins):\n",
    "\n",
    "    mask = Z > H_Arr[iii]\n",
    "\n",
    "    fact_up_Arr[iii] = np.sum(Z[ mask ]) / TOTAL_H\n",
    "\n",
    "H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1sigma\n",
    "H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2sigma\n",
    "H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 2sigma\n",
    "\n",
    "ax.contour(\n",
    "    x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "    colors='C0'\n",
    ")\n",
    "\n",
    "Z, x, y = np.histogram2d(\n",
    "    L_lya[~is_qso & nice_lya & nice_z], L_Arr[~is_qso & nice_lya & nice_z],\n",
    "    bins=(np.linspace(42, 47, 30), np.linspace(42, 47, 30))\n",
    ")\n",
    "\n",
    "H_min = np.amin(Z)\n",
    "H_max = np.amax(Z)\n",
    "\n",
    "y_centers = 0.5 * ( y[1:] + y[:-1] )\n",
    "x_centers = 0.5 * ( x[1:] + x[:-1] )\n",
    "\n",
    "N_bins = 10000\n",
    "\n",
    "H_Arr = np.linspace( H_min , H_max , N_bins )[::-1]\n",
    "\n",
    "fact_up_Arr = np.zeros( N_bins )\n",
    "\n",
    "TOTAL_H = np.sum(Z)\n",
    "\n",
    "for iii in range(0, N_bins):\n",
    "\n",
    "    mask = Z > H_Arr[iii]\n",
    "\n",
    "    fact_up_Arr[iii] = np.sum(Z[ mask ]) / TOTAL_H\n",
    "\n",
    "H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1sigma\n",
    "H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2sigma\n",
    "H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 2sigma\n",
    "\n",
    "ax.contour(\n",
    "    x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "    colors='C1'\n",
    ")\n",
    "\n",
    "# ax.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "# ax.scatter(L_lya[nice_lya & is_qso], L_Arr[nice_lya & is_qso],\n",
    "#     label='QSO', alpha=0.3)\n",
    "# ax.scatter(L_lya[nice_lya & ~is_qso], L_Arr[nice_lya & ~is_qso],\n",
    "#     label='SF', alpha=0.3)\n",
    "\n",
    "x = np.linspace(40, 48, 100)\n",
    "ax.plot(x, x, linestyle='--', color='red', label='1:1')\n",
    "\n",
    "ax.set_ylabel('Retrieved $\\log L$', fontsize=15)\n",
    "ax.set_xlabel('Real $\\log L$', fontsize=15)\n",
    "\n",
    "ax.set_ylim((42, 47))\n",
    "ax.set_xlim((42, 47))\n",
    "\n",
    "ax.legend(fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "N_bins_1 = 6\n",
    "N_bins_2 = 6\n",
    "bins = np.concatenate((\n",
    "    np.linspace(43.2, 44.25, N_bins_1 + 1),\n",
    "    np.linspace(44.25, 45.5, N_bins_2 + 1)[1:]\n",
    "))\n",
    "bin_centers = [(bins[k] + bins[k + 1]) / 2 for k in range(len(bins) - 1)]\n",
    "\n",
    "goodh = L_Arr[nice_lya & nice_z]\n",
    "badh = L_Arr[nice_lya & ~nice_z]\n",
    "\n",
    "hg, bg = np.histogram(goodh, bins=bins)\n",
    "hb, _ = np.histogram(badh, bins=bins)\n",
    "\n",
    "phistar1 = 3.33e-6\n",
    "Lstar1 = 10 ** 44.65\n",
    "alpha1 = -1.35\n",
    "phistar2 = 10 ** -3.45\n",
    "Lstar2 = 10 ** 42.93\n",
    "alpha2 = -1.93\n",
    "\n",
    "volume = z_volume(z_min, z_max, 200)\n",
    "\n",
    "bins2 = np.linspace(42, 46, 30)\n",
    "b_c2 = [(bins2[i] + bins2[i + 1]) * 0.5 for i in range(len(bins2) - 1)]\n",
    "bw2 = bins2[1] - bins2[0]\n",
    "h_qso, _ = np.histogram(L_lya[is_qso & zspec_cut & ew_cut & mag_cut], bins2)\n",
    "h_sf, _ = np.histogram(L_lya[~is_qso & zspec_cut & ew_cut & mag_cut], bins2)\n",
    "\n",
    "b_c = [0.5 * (bins[i] + bins[i + 1]) for i in range(len(bins) - 1)]\n",
    "bw = [bins[i + 1] - bins[i] for i in range(len(bins) - 1)]\n",
    "\n",
    "totals = []\n",
    "for b_i, _ in enumerate(b_c):\n",
    "    Lx = np.linspace(bins[b_i], bins[b_i + 1], 100)\n",
    "\n",
    "    totals.append(\n",
    "        simpson(\n",
    "            np.interp(\n",
    "                Lx, b_c2, (h_qso + h_sf) / bw2\n",
    "            ),\n",
    "            Lx\n",
    "        )\n",
    "    )\n",
    "\n",
    "totals = np.array(totals)\n",
    "\n",
    "ax.plot(b_c, hg / totals, marker='s', label='Completeness')\n",
    "ax.plot(b_c, hg / (hg + hb), marker='s', label='Purity')\n",
    "# ax.step(bin_centers, hg / (hg + hb) / (hg / totals))\n",
    "\n",
    "ax.set_xlabel('$\\log L$', fontsize=15)\n",
    "\n",
    "ax.set_xlim((43.2, 45.5))\n",
    "ax.set_ylim((0, 1))\n",
    "ax.legend(fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_2d_puricomp(L_Arr, L_lya, mag, L_bins):\n",
    "    fig = plt.figure(figsize=(7, 6))\n",
    "\n",
    "    height = 1.\n",
    "    width = 1.\n",
    "    height2 = 0.7\n",
    "    spacing = 0.1\n",
    "    cbar_width = 0.06\n",
    "\n",
    "    ax0 = fig.add_axes([0, height2 + spacing, width, height])\n",
    "    ax1 = fig.add_axes([width + spacing, height2 + spacing, width, height])\n",
    "    axc = fig.add_axes([2 * width + 1.5 * spacing, height2 + spacing, cbar_width, height])\n",
    "    ax2 = fig.add_axes([0, 0, width, height2])\n",
    "    ax3 = fig.add_axes([width + spacing, 0, width, height2])\n",
    "\n",
    "    L_bins = np.linspace(42.5, 46, 20)\n",
    "    r_bins = np.linspace(mag_min, mag_max, 20)\n",
    "\n",
    "    L_bins_c = np.array([0.5 * (L_bins[i] + L_bins[i + 1]) for i in range(len(L_bins) - 1)])\n",
    "\n",
    "    # Perturb L\n",
    "    \n",
    "    N_iter = 1000\n",
    "    h2d_nice_i = np.empty((len(L_bins) - 1, len(r_bins) - 1, N_iter))\n",
    "    h2d_sel_i = np.empty((len(L_bins) - 1, len(r_bins) - 1, N_iter))\n",
    "    h1d_nice_i = np.empty((len(L_bins) - 1, N_iter))\n",
    "    h1d_sel_i = np.empty((len(L_bins) - 1, N_iter))\n",
    "\n",
    "    L_binning_position = binned_statistic(\n",
    "        L_Arr, None, 'count', bins=L_binning\n",
    "    ).binnumber\n",
    "    L_binning_position[L_binning_position > len(L_binning) - 2] = len(L_binning) - 2\n",
    "\n",
    "    L_err = L_Lbin_err[:, L_binning_position - 1]\n",
    "\n",
    "    for k in range(N_iter):\n",
    "        randN = np.random.randn(N_sources)\n",
    "        L_perturbation = np.zeros(N_sources)\n",
    "        L_perturbation[randN >= 0.] = L_err[0, randN >= 0.] * randN[randN >= 0.]\n",
    "        L_perturbation[randN < 0.] = L_err[1, randN < 0.] * randN[randN < 0.]\n",
    "\n",
    "        L_perturbed = L_Arr + L_perturbation\n",
    "        L_perturbed[np.isnan(L_perturbed)] = 0.\n",
    "\n",
    "        h2d_nice_i[..., k], _, _ = np.histogram2d(\n",
    "            L_perturbed[nice_lya & nice_z],\n",
    "            mag[nice_lya & nice_z],\n",
    "            bins=[L_bins, r_bins]\n",
    "        )\n",
    "\n",
    "        h2d_sel_i[..., k], _, _ = np.histogram2d(\n",
    "            L_perturbed[nice_lya],\n",
    "            mag[nice_lya],\n",
    "            bins=[L_bins, r_bins]\n",
    "        )\n",
    "\n",
    "        h1d_nice_i[..., k], _ = np.histogram(L_perturbed[nice_lya & nice_z], bins=L_bins)\n",
    "        h1d_sel_i[..., k], _ = np.histogram(L_perturbed[nice_lya], bins=L_bins)\n",
    "\n",
    "    # Take the median\n",
    "    h2d_nice = np.median(h2d_nice_i, axis=2)\n",
    "    h2d_sel = np.median(h2d_sel_i, axis=2)\n",
    "    h1d_nice = np.median(h1d_nice_i, axis=1)\n",
    "    h1d_sel = np.median(h1d_sel_i, axis=1)\n",
    "\n",
    "    # h2d_nice, _, _ = np.histogram2d(\n",
    "    #     L_Arr[nice_lya & nice_z],\n",
    "    #     mag[nice_lya & nice_z],\n",
    "    #     bins=[L_bins, r_bins]\n",
    "    # )\n",
    "    # h2d_sel, _, _ = np.histogram2d(\n",
    "    #     L_Arr[nice_lya],\n",
    "    #     mag[nice_lya],\n",
    "    #     bins=[L_bins, r_bins]\n",
    "    # )\n",
    "\n",
    "    h2d_parent, _, _ = np.histogram2d(\n",
    "        L_lya[zspec_cut & mag_cut & ew_cut],\n",
    "        mag[zspec_cut & mag_cut & ew_cut],\n",
    "        bins=[L_bins, r_bins]\n",
    "    )\n",
    "    h1d_parent, _ = np.histogram(L_lya[zspec_cut & mag_cut & ew_cut], bins=L_bins)\n",
    "    # h1d_nice, _ = np.histogram(L_Arr[nice_lya & nice_z], bins=L_bins)\n",
    "    # h1d_sel, _ = np.histogram(L_Arr[nice_lya], bins=L_bins)\n",
    "\n",
    "    cmap = 'Spectral'\n",
    "\n",
    "    puri2d = h2d_nice / h2d_sel\n",
    "    comp2d = h2d_nice / h2d_parent\n",
    "    puri1d = h1d_nice / h1d_sel\n",
    "    comp1d = h1d_nice / h1d_parent\n",
    "\n",
    "    puri1d[np.isnan(puri1d)] = 0.\n",
    "    comp1d[np.isnan(comp1d)] = 0.\n",
    "\n",
    "    ### PLOT STUFF\n",
    "\n",
    "    sns.heatmap(puri2d.T, ax=ax0, vmin=0, vmax=1, cbar_ax=axc, cmap=cmap)\n",
    "    sns.heatmap(comp2d.T, ax=ax1, vmin=0, vmax=1, cbar=False, cmap=cmap)\n",
    "\n",
    "    ax2.plot(L_bins_c, puri1d, marker='s')\n",
    "    ax3.plot(L_bins_c, comp1d, marker='s')\n",
    "\n",
    "    ### TICKS\n",
    "\n",
    "    xticks = range(len(L_bins))\n",
    "    yticks = range(len(r_bins))\n",
    "    xtick_labels = ['{0:0.1f}'.format(n) for n in L_bins]\n",
    "    ytick_labels = ['{0:0.1f}'.format(n) for n in r_bins]\n",
    "\n",
    "\n",
    "    ax0.set_yticks(yticks)\n",
    "    ax0.set_yticklabels(ytick_labels, rotation='horizontal')\n",
    "    ax0.set_xticks(xticks)\n",
    "    ax0.set_xticklabels(xtick_labels, rotation='vertical')\n",
    "    ax0.yaxis.set_ticks_position('both')\n",
    "    ax0.xaxis.set_ticks_position('both')\n",
    "    ax0.tick_params(axis='y', direction='in', labelsize=14)\n",
    "    ax0.tick_params(axis='x', direction='in', labelsize=14)\n",
    "\n",
    "    ax1.set_yticks(yticks)\n",
    "    ax1.set_yticklabels(ytick_labels, rotation='horizontal')\n",
    "    ax1.set_xticks(xticks)\n",
    "    ax1.set_xticklabels(xtick_labels, rotation='vertical')\n",
    "    ax1.yaxis.set_ticks_position('both')\n",
    "    ax1.xaxis.set_ticks_position('both')\n",
    "    ax1.tick_params(axis='y', direction='in', labelsize=14)\n",
    "    ax1.tick_params(axis='x', direction='in', labelsize=14)\n",
    "\n",
    "    axc.tick_params(labelsize=14)\n",
    "    ax2.tick_params(labelsize=14)\n",
    "    ax3.tick_params(labelsize=14)\n",
    "\n",
    "    ## Spines\n",
    "    ax0.spines[:].set_visible(True)\n",
    "    ax1.spines[:].set_visible(True)\n",
    "\n",
    "    ## Axis lims\n",
    "    ax2.set_ylim((0, 1))\n",
    "    # ax3.set_ylim((0, 1))\n",
    "\n",
    "    ## Titles\n",
    "    ax0.set_title('Purity', fontsize=25)\n",
    "    ax1.set_title('Completeness', fontsize=25)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    np.save('npy/puri2d.npy', puri2d)\n",
    "    np.save('npy/comp2d.npy', comp2d)\n",
    "    np.save('npy/puricomp2d_L_bins.npy', L_bins)\n",
    "    np.save('npy/puricomp2d_r_bins.npy', r_bins)\n",
    "\n",
    "    return puri2d, comp2d, L_bins, r_bins\n",
    "puri2d, comp2d, L_bins, r_bins = make_2d_puricomp(L_Arr, L_lya, mag, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lya_intrisic_completeness(L, z, starprob=None):\n",
    "    if starprob == None:\n",
    "        starprob = np.zeros(L.shape)\n",
    "\n",
    "    isstar = (starprob >= 0.5)\n",
    "\n",
    "    ## MiniJPAS limiting r magnitudes\n",
    "    mag = np.ones(L.shape) * 23.6\n",
    "    mag[~isstar] = 22.7\n",
    "\n",
    "    Fline = 10 ** L / (cosmo.luminosity_distance(z).to(u.cm).value ** 2 * 4*np.pi)\n",
    "    fcont = mag_to_flux(mag, 6750)\n",
    "\n",
    "    EW_max = Fline / fcont / (1 + z)\n",
    "\n",
    "    ew_x = np.linspace(20, 1000, 10000)\n",
    "    w_0 = 75\n",
    "    ew_dist = lambda ew_xx: np.exp(-ew_xx / w_0)\n",
    "\n",
    "    total_ew = simpson(ew_dist(ew_x), ew_x)\n",
    "\n",
    "    completeness = np.empty(L.shape)\n",
    "\n",
    "    for src in range(len(L)):\n",
    "        src_ew_x = np.linspace(20, EW_max[src], 1000)\n",
    "        completeness[src] = simpson(ew_dist(src_ew_x), src_ew_x) / total_ew\n",
    "\n",
    "    return completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intcomp = Lya_intrisic_completeness(L_Arr[nice_lya], z_Arr[nice_lya], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection algorithm weights\n",
    "# weights = puricomp2d_weights(L_Arr[nice_lya], mag[nice_lya], puri2d, comp2d, L_bins, r_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_perturb_err(L_Arr, L_e_Arr, nice_lya, bins, L_binning):\n",
    "    N_bins = len(bins) - 1\n",
    "\n",
    "    hist_i_mat = np.zeros((1000, N_bins))\n",
    "\n",
    "    L_binning_position = binned_statistic(\n",
    "        L_Arr, None, 'count', bins=L_binning\n",
    "    ).binnumber\n",
    "    L_binning_position[L_binning_position > len(L_binning) - 2] = len(L_binning) - 2\n",
    "\n",
    "    L_err = L_Lbin_err[:, L_binning_position - 1]\n",
    "\n",
    "    for k in range(1000):\n",
    "        randN = np.random.randn(N_sources)\n",
    "        L_perturbation = np.zeros(N_sources)\n",
    "        L_perturbation[randN >= 0.] = L_err[0, randN >= 0.] * randN[randN >= 0.]\n",
    "        L_perturbation[randN < 0.] = L_err[1, randN < 0.] * randN[randN < 0.]\n",
    "\n",
    "        L_perturbed = L_Arr + L_perturbation\n",
    "        L_perturbed[np.isnan(L_perturbed)] = 0.\n",
    "\n",
    "        w = puricomp2d_weights(\n",
    "            L_perturbed[nice_lya], mag[nice_lya], puri2d, comp2d, L_bins, r_bins\n",
    "        )\n",
    "        # w = weights_LF(\n",
    "        #     L_perturbed[nice_lya], mag[nice_lya], puri2d, comp2d, L_bins, r_bins,\n",
    "        #     z_Arr[nice_lya], None, None\n",
    "        # )\n",
    "        hist = np.histogram(L_perturbed[nice_lya], bins=bins)[0]\n",
    "        hist_poiss_err = np.round(\n",
    "            hist[0] ** 0.5 * np.random.randn(len(bins) - 1), 0\n",
    "        ).astype(int)\n",
    "\n",
    "        hist_binnumber = binned_statistic(L_perturbed[nice_lya], None, 'count', bins=bins)[2]\n",
    "\n",
    "        L_Arr_to_hist = np.array([])\n",
    "        w_Arr_to_hist = np.array([])\n",
    "        for bin in range(N_bins):\n",
    "            where_bin = np.where(hist_binnumber == bin + 1)[0]\n",
    "            try:\n",
    "                idx = np.random.choice(\n",
    "                    where_bin, size=(hist_poiss_err[bin] + hist[bin]),\n",
    "                    replace=True\n",
    "                )\n",
    "                L_Arr_to_hist = np.hstack([L_Arr_to_hist, L_perturbed[nice_lya][idx]])\n",
    "                w_Arr_to_hist = np.hstack([w_Arr_to_hist, w[idx]])\n",
    "            except:\n",
    "                pass\n",
    "        hist_i_mat[k], _ = np.histogram(L_Arr_to_hist, bins=bins, weights=w_Arr_to_hist)\n",
    "\n",
    "    L_LF_err_percentiles = np.percentile(hist_i_mat, [16, 50, 84], axis=0)\n",
    "    return L_LF_err_percentiles\n",
    "\n",
    "L_e_Arr = np.load('npy/L_nb_err.npy')\n",
    "L_binning = np.load('npy/L_nb_err_binning.npy')\n",
    "\n",
    "L_LF_err_percentiles = LF_perturb_err(L_Arr, L_e_Arr, nice_lya, bins, L_binning)\n",
    "L_LF_err_plus = L_LF_err_percentiles[2] - L_LF_err_percentiles[1]\n",
    "L_LF_err_minus = L_LF_err_percentiles[1] - L_LF_err_percentiles[0]\n",
    "hist_median = L_LF_err_percentiles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_hist_cor_cor, b = np.histogram(L_Arr[nice_lya], bins=bins, weights=weights/intcomp)\n",
    "# total_hist_cor, b = np.histogram(L_Arr[nice_lya], bins=bins, weights=weights)\n",
    "total_hist, b = np.histogram(L_Arr[nice_lya], bins=bins)\n",
    "\n",
    "LF_bins = np.array([(b[i] + b[i + 1]) / 2 for i in range(len(b) - 1)])\n",
    "\n",
    "bin_width = np.array([b[i + 1] - b[i] for i in range(len(b) - 1)])\n",
    "\n",
    "volume = z_volume(z_min, z_max, 200)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "\n",
    "ax.errorbar(LF_bins, total_hist / volume / bw,\n",
    "    yerr=total_hist ** 0.5 / volume / bw,\n",
    "    marker='.', linestyle='', markersize=15, label='Uncorrected')\n",
    "# ax.errorbar(LF_bins, total_hist_cor / volume / bw,\n",
    "#     yerr=(total_hist) ** 0.5 / volume / bw,\n",
    "#     marker='.', linestyle='', markersize=15, label='Corrected')\n",
    "# ax.errorbar(LF_bins, total_hist_cor_cor / volume / bw,\n",
    "#     yerr=(total_hist) ** 0.5 / volume / bw,\n",
    "#     marker='.', linestyle='', markersize=15, label='More corrected')\n",
    "yerr_cor_plus = (hist_median + L_LF_err_plus ** 2) ** 0.5\\\n",
    "     / volume / bin_width\n",
    "yerr_cor_minus = (hist_median + L_LF_err_minus ** 2) ** 0.5\\\n",
    "     / volume / bin_width\n",
    "xerr = bin_width / 2\n",
    "ax.errorbar(LF_bins, hist_median / volume / bin_width,\n",
    "    yerr= [yerr_cor_minus, yerr_cor_plus], xerr=xerr,\n",
    "    marker='.', linestyle='', markersize=15, c='red',\n",
    "    label='Corrected median histogram')\n",
    "\n",
    "bins2 = np.linspace(42, 46, 30)\n",
    "b_c = [(bins2[i] + bins2[i + 1]) * 0.5 for i in range(len(bins2) - 1)]\n",
    "bw = bins2[1] - bins2[0]\n",
    "h_qso, b = np.histogram(L_lya[is_qso & zspec_cut & ew_cut & mag_cut], bins2)\n",
    "h_sf, b = np.histogram(L_lya[~is_qso & zspec_cut & ew_cut & mag_cut], bins2)\n",
    "\n",
    "ax.plot(b_c, (h_qso + h_sf) / bw / volume, c='dimgray', zorder=-99, label='Mock', ls='--')\n",
    "\n",
    "Lx = np.linspace(10 ** 42, 10 ** 45.5, 1000)\n",
    "Phi = double_schechter(\n",
    "                Lx, phistar2, Lstar2, alpha2, phistar1, Lstar1, alpha1\n",
    "            ) * Lx * np.log(10)\n",
    "\n",
    "plt.plot(np.log10(Lx), Phi)\n",
    "\n",
    "ax.set_ylim(1e-9, 1e-3)\n",
    "ax.set_xlim(42.75, 45.5)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.legend(fontsize=15)\n",
    "\n",
    "ax.set_xlabel('log L', fontsize=15)\n",
    "ax.set_ylabel('$\\Phi$ [Mpc$^{-3}$ $\\Delta$ log L $^{-1}$]', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_hist)\n",
    "# print(total_hist_cor)\n",
    "# print(total_hist_cor_cor)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ade4bca3e0042e0da78fecdb82351169c0f2ccedb06a0d7cf7342df8f7e47af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
