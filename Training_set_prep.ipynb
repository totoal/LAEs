{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from my_functions import *\n",
    "from load_mocks import load_QSO_mock, load_SF_mock\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_central = central_wavelength()\n",
    "nb_fwhm_Arr = nb_fwhm(range(60))\n",
    "w_lya = 1215.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_lya_search(flx, err, L_lya, mag_min, mag_max, is_qso):\n",
    "    # Lya search\n",
    "    cont_est_lya, cont_err_lya = estimate_continuum(flx, err, IGM_T_correct=True)\n",
    "    line = is_there_line(flx, err, cont_est_lya, cont_err_lya, 30)\n",
    "    lya_lines, lya_cont_lines, _ = identify_lines(\n",
    "        line, flx, err, first=True, return_line_width=True\n",
    "    )\n",
    "    lya_lines = np.array(lya_lines)\n",
    "\n",
    "    # Other lines\n",
    "    cont_est_other, cont_err_other = estimate_continuum(flx, err, IGM_T_correct=False)\n",
    "    line_other = is_there_line(flx, err, cont_est_other, cont_err_other,\n",
    "        400, obs=True)\n",
    "    other_lines = identify_lines(line_other, flx, err)\n",
    "\n",
    "    # Compute z\n",
    "    N_sources = flx.shape[1]\n",
    "    z_Arr = np.zeros(N_sources)\n",
    "    z_Arr[np.where(np.array(lya_lines) != -1)] =\\\n",
    "        z_NB(np.array(lya_cont_lines)[np.where(np.array(lya_lines) != -1)])\n",
    "\n",
    "    nb_min = 3\n",
    "    nb_max = 20\n",
    "\n",
    "    z_min = (w_central[nb_min] - nb_fwhm_Arr[nb_min] * 0.5) / w_lya - 1\n",
    "    z_max = (w_central[nb_max] + nb_fwhm_Arr[nb_max] * 0.5) / w_lya - 1\n",
    "\n",
    "    z_cut = (z_min < z_Arr) & (z_Arr < z_max)\n",
    "\n",
    "    mag = flux_to_mag(flx[-2].astype(float), float(w_central[-2]))\n",
    "    mag[np.isnan(mag)] = 99.\n",
    "\n",
    "    nice_lya = nice_lya_select(\n",
    "        lya_lines, other_lines, flx, err, cont_est_lya, z_Arr\n",
    "    )\n",
    "\n",
    "    nice_lya = (\n",
    "        z_cut & (mag > mag_min) & (mag < mag_max) & nice_lya\n",
    "        # & (L_lya > 43 if is_qso else L_lya > 0)\n",
    "    )\n",
    "\n",
    "    _, _, L_Arr, _, _, _ = EW_L_NB(\n",
    "        flx, err, cont_est_lya, cont_err_lya, z_Arr, lya_lines, N_nb=0\n",
    "    )\n",
    "\n",
    "    return nice_lya, z_Arr, L_Arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sources(flx, err, L_lya, mag_min, mag_max, is_qso, N_samples=100_000):\n",
    "    out_flx = np.array([])\n",
    "    n_iter = 0\n",
    "    while True:\n",
    "        n_iter += 1\n",
    "        this_flx = flx + err * np.random.normal(size=err.shape)\n",
    "\n",
    "        this_flx[err > 1] = 0.\n",
    "\n",
    "        this_nice_lya, this_z_Arr, this_L_Arr = nice_lya_search(\n",
    "            this_flx, err, L_lya, mag_min, mag_max, is_qso\n",
    "        )\n",
    "\n",
    "        if len(out_flx) == 0:\n",
    "            out_flx = flx[:, this_nice_lya]\n",
    "            out_err = err[:, this_nice_lya]\n",
    "            out_z = this_z_Arr[this_nice_lya]\n",
    "            out_L = this_L_Arr[this_nice_lya]\n",
    "            out_L_lya = L_lya[this_nice_lya]\n",
    "        else:\n",
    "            out_flx = np.hstack((out_flx, this_flx[:, this_nice_lya]))\n",
    "            out_err = np.hstack((out_err, err[:, this_nice_lya]))\n",
    "            out_z = np.concatenate((out_z, this_z_Arr[this_nice_lya]))\n",
    "            out_L = np.concatenate((out_L, this_L_Arr[this_nice_lya]))\n",
    "            out_L_lya = np.concatenate((out_L_lya, L_lya[this_nice_lya]))\n",
    "        \n",
    "        print(f'Sampled {len(out_z)} / {N_samples}')\n",
    "        \n",
    "        if len(out_z) >= N_samples:\n",
    "            break\n",
    "        if len(out_z) == 0:\n",
    "            break\n",
    "\n",
    "    if len(out_z) == 0:\n",
    "        return out_flx, out_err, out_z, out_L, out_L_lya\n",
    "\n",
    "    randomize = np.random.choice(np.arange(0, len(out_L)), N_samples)\n",
    "\n",
    "    out_flx = out_flx[:, randomize]\n",
    "    out_err = out_err[:, randomize]\n",
    "    out_z = out_z[randomize]\n",
    "    out_L = out_L[randomize]\n",
    "    out_L_lya = out_L_lya[randomize]\n",
    "\n",
    "    return out_flx, out_err, out_z, out_L, out_L_lya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_dataset(N_samples, mag_min, mag_max):\n",
    "    N_sources_sf = sf_flx.shape[1]\n",
    "    random_perm_sf = np.random.permutation(np.arange(N_sources_sf))\n",
    "    N_sources_qso = qso_flx.shape[1]\n",
    "    random_perm_qso = np.random.permutation(np.arange(N_sources_qso))\n",
    "\n",
    "    dataset = np.array([]).reshape(0, 114)\n",
    "    L_labels = np.array([])\n",
    "\n",
    "    n_folds = 3\n",
    "    N_sources_k_sf = N_sources_sf // n_folds\n",
    "    N_sources_k_qso = N_sources_qso // n_folds\n",
    "\n",
    "    N_samples_k = N_samples // n_folds\n",
    "    for k in range(n_folds):\n",
    "        print(f'Fold #{k + 1}')\n",
    "        fold_idx_sf = random_perm_sf[k * N_sources_k_sf : (k + 1) * N_sources_k_sf]\n",
    "        fold_idx_qso = random_perm_qso[k * N_sources_k_qso : (k + 1) * N_sources_k_qso]\n",
    "\n",
    "        sf_flx_data, sf_err_data, sf_z_data, sf_L_data, sf_L_Lya_data =\\\n",
    "            sample_sources(\n",
    "                sf_flx[:, fold_idx_sf], sf_err[:, fold_idx_sf], sf_L[fold_idx_sf],\n",
    "                mag_min, mag_max, False, N_samples_k // 2\n",
    "            )\n",
    "        qso_flx_data, qso_err_data, qso_z_data, qso_L_data, qso_L_Lya_data =\\\n",
    "            sample_sources(\n",
    "                qso_flx[:, fold_idx_qso], qso_err[:, fold_idx_qso], qso_L[fold_idx_qso],\n",
    "                mag_min, mag_max, True, N_samples_k // 2\n",
    "            )\n",
    "\n",
    "        # pm_flx = np.hstack((qso_flx_data, sf_flx_data))\n",
    "        # pm_err = np.hstack((qso_err_data, sf_err_data))\n",
    "        # z_Arr = np.concatenate((qso_z_data, sf_z_data))\n",
    "        # L_Arr = np.concatenate((qso_L_data, sf_L_data))\n",
    "        pm_flx = qso_flx_data\n",
    "        pm_err = qso_err_data\n",
    "        z_Arr = qso_z_data\n",
    "        L_Arr = qso_L_data\n",
    "\n",
    "        # L_labels_k = np.concatenate((qso_L_Lya_data, sf_L_Lya_data))\n",
    "        L_labels_k = qso_L_Lya_data\n",
    "\n",
    "        dataset_k = np.hstack(\n",
    "            (\n",
    "                pm_flx[2:55].T,\n",
    "                pm_flx[-3:].T,\n",
    "                np.abs(pm_err[2:55].T / pm_flx[2:55].T),\n",
    "                np.abs(pm_err[-3:].T / pm_flx[-3:].T),\n",
    "                L_Arr.reshape(-1, 1),\n",
    "                z_Arr.reshape(-1, 1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        dataset = np.vstack((dataset, dataset_k))\n",
    "        L_labels = np.concatenate((L_labels, L_labels_k))\n",
    "\n",
    "    return dataset, L_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qso_name = 'QSO_double_train_minijpas_DR16_0'\n",
    "sf_name = 'LAE_12.5deg_z2-4.25_train_minijpas_0'\n",
    "qso_flx, qso_err, EW_qso, qso_zspec, qso_L = load_QSO_mock(qso_name, add_errs=False)\n",
    "sf_flx, sf_err, EW_sf, sf_zspec, sf_L = load_SF_mock(sf_name, add_errs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(qso_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_min_list = [15]\n",
    "mag_max_list = [24]\n",
    "\n",
    "for mag_min, mag_max in zip(mag_min_list, mag_max_list):\n",
    "    print(f'Ensembling dataset mag{mag_min}-{mag_max}')\n",
    "    dataset, L_tags = ensemble_dataset(10_000, mag_min, mag_max)\n",
    "\n",
    "    pd.DataFrame(dataset).to_csv(f'MLmodels/datasets/dataset_mag{mag_min}-{mag_max}_train.csv')\n",
    "    pd.DataFrame(L_tags).to_csv(f'MLmodels/datasets/tags_mag{mag_min}-{mag_max}_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qso_name = 'QSO_double_test_minijpas_DR16_0'\n",
    "sf_name = 'LAE_12.5deg_z2-4.25_test_minijpas_0'\n",
    "qso_flx, qso_err, EW_qso, qso_zspec, qso_L = load_QSO_mock(qso_name, add_errs=False)\n",
    "sf_flx, sf_err, EW_sf, sf_zspec, sf_L = load_SF_mock(sf_name, add_errs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_min_list = [15]\n",
    "mag_max_list = [24]\n",
    "\n",
    "for mag_min, mag_max in zip(mag_min_list, mag_max_list):\n",
    "    print(f'Ensembling dataset mag{mag_min}-{mag_max}')\n",
    "    dataset, L_tags = ensemble_dataset(10_000, mag_min, mag_max)\n",
    "\n",
    "    pd.DataFrame(dataset).to_csv(f'MLmodels/datasets/dataset_mag{mag_min}-{mag_max}_test.csv')\n",
    "    pd.DataFrame(L_tags).to_csv(f'MLmodels/datasets/tags_mag{mag_min}-{mag_max}_test.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ade4bca3e0042e0da78fecdb82351169c0f2ccedb06a0d7cf7342df8f7e47af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
