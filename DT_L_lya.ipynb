{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pickle\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_central = central_wavelength()\n",
    "nb_fwhm_Arr = nb_fwhm(range(60))\n",
    "w_lya = 1215.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The initial features are:\n",
    "- The fluxes of the first 55 NBs\n",
    "- The errors of the first 55 NBs\n",
    "- 4 BB fluxes\n",
    "- 4 BB errors\n",
    "- The estimated L\n",
    "- The estimated z\n",
    "TOTAL = 120 features\n",
    "(PCA to be applied below)\n",
    "'''\n",
    "\n",
    "# The data set is the nice_lya sample\n",
    "NNdata = pd.read_csv('MLmodels/dataset100_000.csv').to_numpy()\n",
    "NNdata_L_input = pd.read_csv('MLmodels/tags100_000.csv').to_numpy()\n",
    "\n",
    "N_sources_NN = NNdata.shape[0]\n",
    "\n",
    "is_qso = np.ones(N_sources_NN).astype(bool)\n",
    "\n",
    "# Shuffle data\n",
    "shuffle_idx = np.random.permutation(np.arange(N_sources_NN))\n",
    "NNdata = NNdata[shuffle_idx]\n",
    "\n",
    "NNdata_L_input = NNdata_L_input[shuffle_idx]\n",
    "\n",
    "# Take logs\n",
    "NNdata[:, :55 + 4] = np.log10(NNdata[:, :55 + 4])\n",
    "\n",
    "NNdata[np.isnan(NNdata)] = -99.\n",
    "\n",
    "# Rescale data\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(NNdata)\n",
    "NNdata = mms.transform(NNdata)\n",
    "with open('MLmodels/DT_QSO-SF_scaler.sav', 'wb') as file:\n",
    "    pickle.dump(mms, file)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95, svd_solver='full')\n",
    "\n",
    "pca.fit(NNdata)\n",
    "with open('MLmodels/DT_QSO-SF_pca.sav', 'wb') as file:\n",
    "    pickle.dump(pca, file)\n",
    "\n",
    "NNdata = pca.transform(NNdata)\n",
    "\n",
    "# Split dataset\n",
    "NNdata_train, NNdata_test, NNlabels_train, NNlabels_test =\\\n",
    "    train_test_split(NNdata, NNdata_L_input, test_size=0.2, shuffle=False)\n",
    "\n",
    "N_train = len(NNlabels_train)\n",
    "N_test = len(NNlabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the parameter grid based on the results of random search \n",
    "# param_grid = {\n",
    "#     'bootstrap': [False, True],\n",
    "#     'max_depth': [40, 50, 60],\n",
    "#     'max_features': ['sqrt'],\n",
    "#     'min_samples_leaf': [2, 3, 4],\n",
    "#     'min_samples_split': [3, 4, 5],\n",
    "#     'n_estimators': [200, 250]\n",
    "# }\n",
    "# # Create a based model\n",
    "# rf = RandomForestRegressor()\n",
    "# # Instantiate the grid search model\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=rf, param_grid=param_grid, \n",
    "#     cv=3, n_jobs=-1, verbose=2, refit=True\n",
    "# )\n",
    "\n",
    "# reg = grid_search.fit(NNdata_train, NNdata_L_input[:N_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_lya_test = NNdata_L_input[N_train:]\n",
    "\n",
    "# The regressor\n",
    "best_params = {\n",
    "    'bootstrap': False,\n",
    "    'max_depth': 50,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 3,\n",
    "    'min_samples_split': 4,\n",
    "    'n_estimators': 200,\n",
    "    'verbose': True,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "reg = RandomForestRegressor(**best_params)\n",
    "\n",
    "# Train it\n",
    "reg.fit(NNdata_train, NNdata_L_input[:N_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Arr_pred = reg.predict(NNdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MLmodels/DT_QSO-SF_regressor.sav', 'wb') as file:\n",
    "    pickle.dump(reg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(maskkk, is_qso, title='', nb_c=-3):\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "    mask = maskkk[is_qso]\n",
    "    Z, x, y = np.histogram2d(\n",
    "        L_lya_test[is_qso][mask], L_Arr_pred[is_qso][mask],\n",
    "        bins=(np.linspace(41, 47, 30), np.linspace(41, 47, 30))\n",
    "    )\n",
    "\n",
    "    H_min = np.amin(Z)\n",
    "    H_max = np.amax(Z)\n",
    "\n",
    "    y_centers = 0.5 * (y[1:] + y[:-1])\n",
    "    x_centers = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "    N_bins = 10000\n",
    "\n",
    "    H_Arr = np.linspace(H_min, H_max, N_bins)[::-1]\n",
    "\n",
    "    fact_up_Arr = np.zeros(N_bins)\n",
    "\n",
    "    TOTAL_H = np.sum(Z)\n",
    "\n",
    "    for iii in range(0, N_bins):\n",
    "\n",
    "        mask = Z > H_Arr[iii]\n",
    "\n",
    "        fact_up_Arr[iii] = np.sum(Z[mask]) / TOTAL_H\n",
    "\n",
    "    H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1 sigma\n",
    "    H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2 sigma\n",
    "    H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 3 sigma\n",
    "\n",
    "    ax.contour(\n",
    "        x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "        colors='C0'\n",
    "    )\n",
    "\n",
    "    mask = ~is_qso & maskkk\n",
    "    Z, x, y = np.histogram2d(\n",
    "        L_lya_test[mask], L_Arr_pred[mask],\n",
    "        bins=(np.linspace(41, 47, 30), np.linspace(41, 47, 30))\n",
    "    )\n",
    "\n",
    "    H_min = np.amin(Z)\n",
    "    H_max = np.amax(Z)\n",
    "\n",
    "    y_centers = 0.5 * (y[1:] + y[:-1])\n",
    "    x_centers = 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "    N_bins = 10000\n",
    "\n",
    "    H_Arr = np.linspace(H_min , H_max , N_bins )[::-1]\n",
    "\n",
    "    fact_up_Arr = np.zeros(N_bins)\n",
    "\n",
    "    TOTAL_H = np.sum(Z)\n",
    "\n",
    "    for iii in range(0, N_bins):\n",
    "\n",
    "        mask = Z > H_Arr[iii]\n",
    "\n",
    "        fact_up_Arr[iii] = np.sum(Z[ mask ]) / TOTAL_H\n",
    "\n",
    "    H_value_68 = np.interp(0.683, fact_up_Arr, H_Arr) # 1sigma\n",
    "    H_value_95 = np.interp(0.954, fact_up_Arr, H_Arr) # 2sigma\n",
    "    H_value_99 = np.interp(0.997, fact_up_Arr, H_Arr) # 2sigma\n",
    "\n",
    "    ax.contour(\n",
    "        x_centers, y_centers, Z.T, levels=[H_value_99, H_value_95, H_value_68],\n",
    "        colors='C1'\n",
    "    )\n",
    "\n",
    "    x = np.linspace(40, 48, 100)\n",
    "    ax.plot(x, x, linestyle='--', color='red', label='1:1')\n",
    "\n",
    "    ax.set_ylabel('Retrieved $\\log L$', fontsize=15)\n",
    "    ax.set_xlabel('Real $\\log L$', fontsize=15)\n",
    "\n",
    "    ax.set_ylim((41, 47))\n",
    "    ax.set_xlim((41, 47))\n",
    "\n",
    "    if len(title) > 0:\n",
    "        ax.set_title(title, fontsize=20)\n",
    "\n",
    "    # Detec lim\n",
    "\n",
    "    detec_lim = np.vstack(\n",
    "        (\n",
    "            pd.read_csv('csv/5sigma_depths_NB.csv', header=None),\n",
    "            pd.read_csv('csv/5sigma_depths_BB.csv', header=None)\n",
    "        )\n",
    "    )[:, 1]\n",
    "\n",
    "    flambda_lim = mag_to_flux(detec_lim[nb_c], w_central[nb_c]) * 3\n",
    "\n",
    "    ew0_lim = 20 # A\n",
    "    z = w_central[nb_c] / 1215.67 - 1\n",
    "    Fline_lim = ew0_lim * flambda_lim * (1 + z)\n",
    "    dL = cosmo.luminosity_distance(z).to(u.cm).value\n",
    "    L_lim = np.log10(Fline_lim * 4*np.pi * dL**2)\n",
    "\n",
    "    ax.axhline(L_lim, ls='--', color='green', label='L limit')\n",
    "\n",
    "    ax.legend(fontsize=15)\n",
    "    # plt.savefig(f'/home/alberto/Desktop/{title}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(L_Arr_pred.astype(bool), NNlabels_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ade4bca3e0042e0da78fecdb82351169c0f2ccedb06a0d7cf7342df8f7e47af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
